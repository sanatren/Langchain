{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SImple Genai app using Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "api_key2 = os.getenv(\"OPENAI_API_KEY\")\n",
    "api_langchain = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# ‚úÖ Ensure 'LANGCHAIN_PROJECT' is not None before setting it\n",
    "langchain_project = os.getenv(\"LANGCHAIN_PROJECT\", \"default_project\")  # üëà Provide a default value\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = langchain_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Data ingestion -- from the website\n",
    "from langchain_community.document_loaders import WebBaseLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x109289360>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\"https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\nChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main content\\n\\n\\nBack to top\\n\\n\\n\\n\\nCtrl+K\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Reference\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCtrl+K\\n\\n\\n\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\nX / Twitter\\n\\n\\n\\n\\n\\n\\n\\n\\nCtrl+K\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Reference\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\nX / Twitter\\n\\n\\n\\n\\n\\n\\n\\nSection Navigation\\nBase packages\\n\\nCore\\nagents\\nbeta\\ncaches\\ncallbacks\\nchat_history\\nchat_loaders\\nchat_sessions\\ndocument_loaders\\ndocuments\\nembeddings\\nexample_selectors\\nexceptions\\nglobals\\nindexing\\nlanguage_models\\nload\\nmessages\\noutput_parsers\\noutputs\\nprompt_values\\nprompts\\nBasePromptTemplate\\nAIMessagePromptTemplate\\nBaseChatPromptTemplate\\nBaseMessagePromptTemplate\\nBaseStringMessagePromptTemplate\\nChatMessagePromptTemplate\\nChatPromptTemplate\\nHumanMessagePromptTemplate\\nMessagesPlaceholder\\nSystemMessagePromptTemplate\\nFewShotChatMessagePromptTemplate\\nFewShotPromptTemplate\\nFewShotPromptWithTemplates\\nImagePromptTemplate\\nPromptTemplate\\nStringPromptTemplate\\nStructuredPrompt\\naformat_document\\nformat_document\\nload_prompt\\nload_prompt_from_config\\ncheck_valid_template\\nget_template_variables\\njinja2_formatter\\nmustache_formatter\\nmustache_schema\\nmustache_template_vars\\nvalidate_jinja2\\nPipelinePromptTemplate\\n\\n\\nrate_limiters\\nretrievers\\nrunnables\\nstores\\nstructured_query\\nsys_info\\ntools\\ntracers\\nutils\\nvectorstores\\n\\n\\nLangchain\\nText Splitters\\nCommunity\\nExperimental\\n\\nIntegrations\\n\\nAI21\\nAnthropic\\nAstraDB\\nAWS\\nAzure Dynamic Sessions\\nCerebras\\nChroma\\nCohere\\nDatabricks\\nDeepseek\\nElasticsearch\\nExa\\nFireworks\\nGoogle Community\\nGoogle GenAI\\nGoogle VertexAI\\nGroq\\nHuggingface\\nIBM\\nMilvus\\nMistralAI\\nNeo4J\\nNomic\\nNvidia Ai Endpoints\\nOllama\\nOpenAI\\nPinecone\\nPostgres\\nPrompty\\nQdrant\\nRedis\\nSema4\\nSnowflake\\nSqlserver\\nStandard Tests\\nTogether\\nUnstructured\\nUpstage\\nVoyageAI\\nWeaviate\\nXAI\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Python API Reference\\nlangchain-core: 0.3.33\\nprompts\\nChatPromptTemplate\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nChatPromptTemplate#\\n\\n\\nclass langchain_core.prompts.chat.ChatPromptTemplate[source]#\\nBases: BaseChatPromptTemplate\\nPrompt template for chat models.\\nUse to create flexible templated prompts for chat models.\\nExamples\\n\\nChanged in version 0.2.24: You can pass any Message-like formats supported by\\nChatPromptTemplate.from_messages() directly to ChatPromptTemplate()\\ninit.\\n\\nfrom langchain_core.prompts import ChatPromptTemplate\\n\\ntemplate = ChatPromptTemplate([\\n    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\\n    (\"human\", \"Hello, how are you doing?\"),\\n    (\"ai\", \"I\\'m doing well, thanks!\"),\\n    (\"human\", \"{user_input}\"),\\n])\\n\\nprompt_value = template.invoke(\\n    {\\n        \"name\": \"Bob\",\\n        \"user_input\": \"What is your name?\"\\n    }\\n)\\n# Output:\\n# ChatPromptValue(\\n#    messages=[\\n#        SystemMessage(content=\\'You are a helpful AI bot. Your name is Bob.\\'),\\n#        HumanMessage(content=\\'Hello, how are you doing?\\'),\\n#        AIMessage(content=\"I\\'m doing well, thanks!\"),\\n#        HumanMessage(content=\\'What is your name?\\')\\n#    ]\\n#)\\n\\n\\nMessages Placeholder:\\n\\n# In addition to Human/AI/Tool/Function messages,\\n# you can initialize the template with a MessagesPlaceholder\\n# either using the class directly or with the shorthand tuple syntax:\\n\\ntemplate = ChatPromptTemplate([\\n    (\"system\", \"You are a helpful AI bot.\"),\\n    # Means the template will receive an optional list of messages under\\n    # the \"conversation\" key\\n    (\"placeholder\", \"{conversation}\")\\n    # Equivalently:\\n    # MessagesPlaceholder(variable_name=\"conversation\", optional=True)\\n])\\n\\nprompt_value = template.invoke(\\n    {\\n        \"conversation\": [\\n            (\"human\", \"Hi!\"),\\n            (\"ai\", \"How can I assist you today?\"),\\n            (\"human\", \"Can you make me an ice cream sundae?\"),\\n            (\"ai\", \"No.\")\\n        ]\\n    }\\n)\\n\\n# Output:\\n# ChatPromptValue(\\n#    messages=[\\n#        SystemMessage(content=\\'You are a helpful AI bot.\\'),\\n#        HumanMessage(content=\\'Hi!\\'),\\n#        AIMessage(content=\\'How can I assist you today?\\'),\\n#        HumanMessage(content=\\'Can you make me an ice cream sundae?\\'),\\n#        AIMessage(content=\\'No.\\'),\\n#    ]\\n#)\\n\\n\\n\\nSingle-variable template:\\n\\nIf your prompt has only a single input variable (i.e., 1 instance of ‚Äú{variable_nams}‚Äù),\\nand you invoke the template with a non-dict object, the prompt template will\\ninject the provided argument into that variable location.\\nfrom langchain_core.prompts import ChatPromptTemplate\\n\\ntemplate = ChatPromptTemplate([\\n    (\"system\", \"You are a helpful AI bot. Your name is Carl.\"),\\n    (\"human\", \"{user_input}\"),\\n])\\n\\nprompt_value = template.invoke(\"Hello, there!\")\\n# Equivalent to\\n# prompt_value = template.invoke({\"user_input\": \"Hello, there!\"})\\n\\n# Output:\\n#  ChatPromptValue(\\n#     messages=[\\n#         SystemMessage(content=\\'You are a helpful AI bot. Your name is Carl.\\'),\\n#         HumanMessage(content=\\'Hello, there!\\'),\\n#     ]\\n# )\\n\\n\\n\\nCreate a chat prompt template from a variety of message formats.\\n\\nParameters:\\n\\nmessages ‚Äì sequence of message representations.\\nA message can be represented using the following formats:\\n(1) BaseMessagePromptTemplate, (2) BaseMessage, (3) 2-tuple of\\n(message type, template); e.g., (‚Äúhuman‚Äù, ‚Äú{user_input}‚Äù),\\n(4) 2-tuple of (message class, template), (5) a string which is\\nshorthand for (‚Äúhuman‚Äù, template); e.g., ‚Äú{user_input}‚Äù.\\ntemplate_format ‚Äì format of the template. Defaults to ‚Äúf-string‚Äù.\\ninput_variables ‚Äì A list of the names of the variables whose values are\\nrequired as inputs to the prompt.\\noptional_variables ‚Äì A list of the names of the variables for placeholder\\ninferred (or MessagePlaceholder that are optional. These variables are auto)\\nthem. (from the prompt and user need not provide)\\npartial_variables ‚Äì A dictionary of the partial variables the prompt\\ntemplate carries. Partial variables populate the template so that you\\ndon‚Äôt need to pass them in every time you call the prompt.\\nvalidate_template ‚Äì Whether to validate the template.\\ninput_types ‚Äì A dictionary of the types of the variables the prompt template\\nexpects. If not provided, all variables are assumed to be strings.\\n\\n\\nReturns:\\nA chat prompt template.\\n\\n\\nExamples\\nInstantiation from a list of message templates:\\ntemplate = ChatPromptTemplate([\\n    (\"human\", \"Hello, how are you?\"),\\n    (\"ai\", \"I\\'m doing well, thanks!\"),\\n    (\"human\", \"That\\'s good to hear.\"),\\n])\\n\\n\\nInstantiation from mixed message formats:\\ntemplate = ChatPromptTemplate([\\n    SystemMessage(content=\"hello\"),\\n    (\"human\", \"Hello, how are you?\"),\\n])\\n\\n\\n\\nNote\\nChatPromptTemplate implements the standard Runnable Interface. üèÉ\\nThe Runnable Interface has additional methods that are available on runnables, such as with_types, with_retry, assign, bind, get_graph, and more.\\n\\n\\n\\nparam input_types: Dict[str, Any] [Optional]#\\nA dictionary of the types of the variables the prompt template expects.\\nIf not provided, all variables are assumed to be strings.\\n\\n\\n\\nparam input_variables: list[str] [Required]#\\nA list of the names of the variables whose values are required as inputs to the\\nprompt.\\n\\n\\n\\nparam messages: Annotated[list[MessageLike], SkipValidation()] [Required]#\\nList of messages consisting of either message prompt templates or messages.\\n\\n\\n\\nparam metadata: Dict[str, Any] | None = None#\\nMetadata to be used for tracing.\\n\\n\\n\\nparam optional_variables: list[str] = []#\\noptional_variables: A list of the names of the variables for placeholder\\nor MessagePlaceholder that are optional. These variables are auto inferred\\nfrom the prompt and user need not provide them.\\n\\n\\n\\nparam output_parser: BaseOutputParser | None = None#\\nHow to parse the output of calling an LLM on this formatted prompt.\\n\\n\\n\\nparam partial_variables: Mapping[str, Any] [Optional]#\\nA dictionary of the partial variables the prompt template carries.\\nPartial variables populate the template so that you don‚Äôt need to\\npass them in every time you call the prompt.\\n\\n\\n\\nparam tags: list[str] | None = None#\\nTags to be used for tracing.\\n\\n\\n\\nparam validate_template: bool = False#\\nWhether or not to try validating the template.\\n\\n\\n\\nasync abatch(inputs: list[Input], config: RunnableConfig | list[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None) ‚Üí list[Output]#\\nDefault implementation runs ainvoke in parallel using asyncio.gather.\\nThe default implementation of batch works well for IO bound runnables.\\nSubclasses should override this method if they can batch more efficiently;\\ne.g., if the underlying Runnable uses an API which supports a batch mode.\\n\\nParameters:\\n\\ninputs (list[Input]) ‚Äì A list of inputs to the Runnable.\\nconfig (RunnableConfig | list[RunnableConfig] | None) ‚Äì A config to use when invoking the Runnable.\\nThe config supports standard keys like ‚Äòtags‚Äô, ‚Äòmetadata‚Äô for tracing\\npurposes, ‚Äòmax_concurrency‚Äô for controlling how much work to do\\nin parallel, and other keys. Please refer to the RunnableConfig\\nfor more details. Defaults to None.\\nreturn_exceptions (bool) ‚Äì Whether to return exceptions instead of raising them.\\nDefaults to False.\\nkwargs (Any | None) ‚Äì Additional keyword arguments to pass to the Runnable.\\n\\n\\nReturns:\\nA list of outputs from the Runnable.\\n\\nReturn type:\\nlist[Output]\\n\\n\\n\\n\\n\\nasync abatch_as_completed(inputs: Sequence[Input], config: RunnableConfig | Sequence[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None) ‚Üí AsyncIterator[tuple[int, Output | Exception]]#\\nRun ainvoke in parallel on a list of inputs,\\nyielding results as they complete.\\n\\nParameters:\\n\\ninputs (Sequence[Input]) ‚Äì A list of inputs to the Runnable.\\nconfig (RunnableConfig | Sequence[RunnableConfig] | None) ‚Äì A config to use when invoking the Runnable.\\nThe config supports standard keys like ‚Äòtags‚Äô, ‚Äòmetadata‚Äô for tracing\\npurposes, ‚Äòmax_concurrency‚Äô for controlling how much work to do\\nin parallel, and other keys. Please refer to the RunnableConfig\\nfor more details. Defaults to None. Defaults to None.\\nreturn_exceptions (bool) ‚Äì Whether to return exceptions instead of raising them.\\nDefaults to False.\\nkwargs (Any | None) ‚Äì Additional keyword arguments to pass to the Runnable.\\n\\n\\nYields:\\nA tuple of the index of the input and the output from the Runnable.\\n\\nReturn type:\\nAsyncIterator[tuple[int, Output | Exception]]\\n\\n\\n\\n\\n\\nasync aformat(**kwargs: Any) ‚Üí str#\\nAsync format the chat template into a string.\\n\\nParameters:\\n**kwargs (Any) ‚Äì keyword arguments to use for filling in template variables\\nin all the template messages in this chat template.\\n\\nReturns:\\nformatted string.\\n\\nReturn type:\\nstr\\n\\n\\n\\n\\n\\nasync aformat_messages(**kwargs: Any) ‚Üí list[BaseMessage][source]#\\nAsync format the chat template into a list of finalized messages.\\n\\nParameters:\\n**kwargs (Any) ‚Äì keyword arguments to use for filling in template variables\\nin all the template messages in this chat template.\\n\\nReturns:\\nlist of formatted messages.\\n\\nRaises:\\nValueError ‚Äì If unexpected input.\\n\\nReturn type:\\nlist[BaseMessage]\\n\\n\\n\\n\\n\\nasync aformat_prompt(**kwargs: Any) ‚Üí PromptValue#\\nAsync format prompt. Should return a PromptValue.\\n\\nParameters:\\n**kwargs (Any) ‚Äì Keyword arguments to use for formatting.\\n\\nReturns:\\nPromptValue.\\n\\nReturn type:\\nPromptValue\\n\\n\\n\\n\\n\\nasync ainvoke(input: dict, config: RunnableConfig | None = None, **kwargs: Any) ‚Üí PromptValue#\\nAsync invoke the prompt.\\n\\nParameters:\\n\\ninput (dict) ‚Äì Dict, input to the prompt.\\nconfig (RunnableConfig | None) ‚Äì RunnableConfig, configuration for the prompt.\\nkwargs (Any)\\n\\n\\nReturns:\\nThe output of the prompt.\\n\\nReturn type:\\nPromptValue\\n\\n\\n\\n\\n\\nappend(message: BaseMessagePromptTemplate | BaseMessage | BaseChatPromptTemplate | tuple[str | type, str | list[dict] | list[object]] | str) ‚Üí None[source]#\\nAppend a message to the end of the chat template.\\n\\nParameters:\\nmessage (BaseMessagePromptTemplate | BaseMessage | BaseChatPromptTemplate | tuple[str | type, str | list[dict] | list[object]] | str) ‚Äì representation of a message to append.\\n\\nReturn type:\\nNone\\n\\n\\n\\n\\n\\nasync astream(input: Input, config: RunnableConfig | None = None, **kwargs: Any | None) ‚Üí AsyncIterator[Output]#\\nDefault implementation of astream, which calls ainvoke.\\nSubclasses should override this method if they support streaming output.\\n\\nParameters:\\n\\ninput (Input) ‚Äì The input to the Runnable.\\nconfig (RunnableConfig | None) ‚Äì The config to use for the Runnable. Defaults to None.\\nkwargs (Any | None) ‚Äì Additional keyword arguments to pass to the Runnable.\\n\\n\\nYields:\\nThe output of the Runnable.\\n\\nReturn type:\\nAsyncIterator[Output]\\n\\n\\n\\n\\n\\nasync astream_events(input: Any, config: RunnableConfig | None = None, *, version: Literal[\\'v1\\', \\'v2\\'], include_names: Sequence[str] | None = None, include_types: Sequence[str] | None = None, include_tags: Sequence[str] | None = None, exclude_names: Sequence[str] | None = None, exclude_types: Sequence[str] | None = None, exclude_tags: Sequence[str] | None = None, **kwargs: Any) ‚Üí AsyncIterator[StandardStreamEvent | CustomStreamEvent]#\\nGenerate a stream of events.\\nUse to create an iterator over StreamEvents that provide real-time information\\nabout the progress of the Runnable, including StreamEvents from intermediate\\nresults.\\nA StreamEvent is a dictionary with the following schema:\\n\\n\\nevent: str - Event names are of theformat: on_[runnable_type]_(start|stream|end).\\n\\n\\n\\nname: str - The name of the Runnable that generated the event.\\n\\nrun_id: str - randomly generated ID associated with the given execution ofthe Runnable that emitted the event.\\nA child Runnable that gets invoked as part of the execution of a\\nparent Runnable is assigned its own unique ID.\\n\\n\\n\\n\\nparent_ids: List[str] - The IDs of the parent runnables thatgenerated the event. The root Runnable will have an empty list.\\nThe order of the parent IDs is from the root to the immediate parent.\\nOnly available for v2 version of the API. The v1 version of the API\\nwill return an empty list.\\n\\n\\n\\n\\ntags: Optional[List[str]] - The tags of the Runnable that generatedthe event.\\n\\n\\n\\n\\nmetadata: Optional[Dict[str, Any]] - The metadata of the Runnablethat generated the event.\\n\\n\\n\\ndata: Dict[str, Any]\\n\\nBelow is a table that illustrates some events that might be emitted by various\\nchains. Metadata fields have been omitted from the table for brevity.\\nChain definitions have been included after the table.\\nATTENTION This reference table is for the V2 version of the schema.\\n\\n\\nevent\\nname\\nchunk\\ninput\\noutput\\n\\n\\n\\non_chat_model_start\\n[model name]\\n\\n{‚Äúmessages‚Äù: [[SystemMessage, HumanMessage]]}\\n\\n\\non_chat_model_stream\\n[model name]\\nAIMessageChunk(content=‚Äùhello‚Äù)\\n\\n\\n\\non_chat_model_end\\n[model name]\\n\\n{‚Äúmessages‚Äù: [[SystemMessage, HumanMessage]]}\\nAIMessageChunk(content=‚Äùhello world‚Äù)\\n\\non_llm_start\\n[model name]\\n\\n{‚Äòinput‚Äô: ‚Äòhello‚Äô}\\n\\n\\non_llm_stream\\n[model name]\\n‚ÄòHello‚Äô\\n\\n\\n\\non_llm_end\\n[model name]\\n\\n‚ÄòHello human!‚Äô\\n\\n\\non_chain_start\\nformat_docs\\n\\n\\n\\n\\non_chain_stream\\nformat_docs\\n‚Äúhello world!, goodbye world!‚Äù\\n\\n\\n\\non_chain_end\\nformat_docs\\n\\n[Document(‚Ä¶)]\\n‚Äúhello world!, goodbye world!‚Äù\\n\\non_tool_start\\nsome_tool\\n\\n{‚Äúx‚Äù: 1, ‚Äúy‚Äù: ‚Äú2‚Äù}\\n\\n\\non_tool_end\\nsome_tool\\n\\n\\n{‚Äúx‚Äù: 1, ‚Äúy‚Äù: ‚Äú2‚Äù}\\n\\non_retriever_start\\n[retriever name]\\n\\n{‚Äúquery‚Äù: ‚Äúhello‚Äù}\\n\\n\\non_retriever_end\\n[retriever name]\\n\\n{‚Äúquery‚Äù: ‚Äúhello‚Äù}\\n[Document(‚Ä¶), ..]\\n\\non_prompt_start\\n[template_name]\\n\\n{‚Äúquestion‚Äù: ‚Äúhello‚Äù}\\n\\n\\non_prompt_end\\n[template_name]\\n\\n{‚Äúquestion‚Äù: ‚Äúhello‚Äù}\\nChatPromptValue(messages: [SystemMessage, ‚Ä¶])\\n\\n\\n\\n\\nIn addition to the standard events, users can also dispatch custom events (see example below).\\nCustom events will be only be surfaced with in the v2 version of the API!\\nA custom event has following format:\\n\\n\\nAttribute\\nType\\nDescription\\n\\n\\n\\nname\\nstr\\nA user defined name for the event.\\n\\ndata\\nAny\\nThe data associated with the event. This can be anything, though we suggest making it JSON serializable.\\n\\n\\n\\n\\nHere are declarations associated with the standard events shown above:\\nformat_docs:\\ndef format_docs(docs: List[Document]) -> str:\\n    \\'\\'\\'Format the docs.\\'\\'\\'\\n    return \", \".join([doc.page_content for doc in docs])\\n\\nformat_docs = RunnableLambda(format_docs)\\n\\n\\nsome_tool:\\n@tool\\ndef some_tool(x: int, y: str) -> dict:\\n    \\'\\'\\'Some_tool.\\'\\'\\'\\n    return {\"x\": x, \"y\": y}\\n\\n\\nprompt:\\ntemplate = ChatPromptTemplate.from_messages(\\n    [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\\n).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\\n\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\n\\nasync def reverse(s: str) -> str:\\n    return s[::-1]\\n\\nchain = RunnableLambda(func=reverse)\\n\\nevents = [\\n    event async for event in chain.astream_events(\"hello\", version=\"v2\")\\n]\\n\\n# will produce the following events (run_id, and parent_ids\\n# has been omitted for brevity):\\n[\\n    {\\n        \"data\": {\"input\": \"hello\"},\\n        \"event\": \"on_chain_start\",\\n        \"metadata\": {},\\n        \"name\": \"reverse\",\\n        \"tags\": [],\\n    },\\n    {\\n        \"data\": {\"chunk\": \"olleh\"},\\n        \"event\": \"on_chain_stream\",\\n        \"metadata\": {},\\n        \"name\": \"reverse\",\\n        \"tags\": [],\\n    },\\n    {\\n        \"data\": {\"output\": \"olleh\"},\\n        \"event\": \"on_chain_end\",\\n        \"metadata\": {},\\n        \"name\": \"reverse\",\\n        \"tags\": [],\\n    },\\n]\\n\\n\\nExample: Dispatch Custom Event\\nfrom langchain_core.callbacks.manager import (\\n    adispatch_custom_event,\\n)\\nfrom langchain_core.runnables import RunnableLambda, RunnableConfig\\nimport asyncio\\n\\n\\nasync def slow_thing(some_input: str, config: RunnableConfig) -> str:\\n    \"\"\"Do something that takes a long time.\"\"\"\\n    await asyncio.sleep(1) # Placeholder for some slow operation\\n    await adispatch_custom_event(\\n        \"progress_event\",\\n        {\"message\": \"Finished step 1 of 3\"},\\n        config=config # Must be included for python < 3.10\\n    )\\n    await asyncio.sleep(1) # Placeholder for some slow operation\\n    await adispatch_custom_event(\\n        \"progress_event\",\\n        {\"message\": \"Finished step 2 of 3\"},\\n        config=config # Must be included for python < 3.10\\n    )\\n    await asyncio.sleep(1) # Placeholder for some slow operation\\n    return \"Done\"\\n\\nslow_thing = RunnableLambda(slow_thing)\\n\\nasync for event in slow_thing.astream_events(\"some_input\", version=\"v2\"):\\n    print(event)\\n\\n\\n\\nParameters:\\n\\ninput (Any) ‚Äì The input to the Runnable.\\nconfig (RunnableConfig | None) ‚Äì The config to use for the Runnable.\\nversion (Literal[\\'v1\\', \\'v2\\']) ‚Äì The version of the schema to use either v2 or v1.\\nUsers should use v2.\\nv1 is for backwards compatibility and will be deprecated\\nin 0.4.0.\\nNo default will be assigned until the API is stabilized.\\ncustom events will only be surfaced in v2.\\ninclude_names (Sequence[str] | None) ‚Äì Only include events from runnables with matching names.\\ninclude_types (Sequence[str] | None) ‚Äì Only include events from runnables with matching types.\\ninclude_tags (Sequence[str] | None) ‚Äì Only include events from runnables with matching tags.\\nexclude_names (Sequence[str] | None) ‚Äì Exclude events from runnables with matching names.\\nexclude_types (Sequence[str] | None) ‚Äì Exclude events from runnables with matching types.\\nexclude_tags (Sequence[str] | None) ‚Äì Exclude events from runnables with matching tags.\\nkwargs (Any) ‚Äì Additional keyword arguments to pass to the Runnable.\\nThese will be passed to astream_log as this implementation\\nof astream_events is built on top of astream_log.\\n\\n\\nYields:\\nAn async stream of StreamEvents.\\n\\nRaises:\\nNotImplementedError ‚Äì If the version is not v1 or v2.\\n\\nReturn type:\\nAsyncIterator[StandardStreamEvent | CustomStreamEvent]\\n\\n\\n\\n\\n\\nbatch(inputs: list[Input], config: RunnableConfig | list[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None) ‚Üí list[Output]#\\nDefault implementation runs invoke in parallel using a thread pool executor.\\nThe default implementation of batch works well for IO bound runnables.\\nSubclasses should override this method if they can batch more efficiently;\\ne.g., if the underlying Runnable uses an API which supports a batch mode.\\n\\nParameters:\\n\\ninputs (list[Input])\\nconfig (RunnableConfig | list[RunnableConfig] | None)\\nreturn_exceptions (bool)\\nkwargs (Any | None)\\n\\n\\nReturn type:\\nlist[Output]\\n\\n\\n\\n\\n\\nbatch_as_completed(inputs: Sequence[Input], config: RunnableConfig | Sequence[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None) ‚Üí Iterator[tuple[int, Output | Exception]]#\\nRun invoke in parallel on a list of inputs,\\nyielding results as they complete.\\n\\nParameters:\\n\\ninputs (Sequence[Input])\\nconfig (RunnableConfig | Sequence[RunnableConfig] | None)\\nreturn_exceptions (bool)\\nkwargs (Any | None)\\n\\n\\nReturn type:\\nIterator[tuple[int, Output | Exception]]\\n\\n\\n\\n\\n\\nbind(**kwargs: Any) ‚Üí Runnable[Input, Output]#\\nBind arguments to a Runnable, returning a new Runnable.\\nUseful when a Runnable in a chain requires an argument that is not\\nin the output of the previous Runnable or included in the user input.\\n\\nParameters:\\nkwargs (Any) ‚Äì The arguments to bind to the Runnable.\\n\\nReturns:\\nA new Runnable with the arguments bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\nExample:\\nfrom langchain_community.chat_models import ChatOllama\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nllm = ChatOllama(model=\\'llama2\\')\\n\\n# Without bind.\\nchain = (\\n    llm\\n    | StrOutputParser()\\n)\\n\\nchain.invoke(\"Repeat quoted words exactly: \\'One two three four five.\\'\")\\n# Output is \\'One two three four five.\\'\\n\\n# With bind.\\nchain = (\\n    llm.bind(stop=[\"three\"])\\n    | StrOutputParser()\\n)\\n\\nchain.invoke(\"Repeat quoted words exactly: \\'One two three four five.\\'\")\\n# Output is \\'One two\\'\\n\\n\\n\\n\\n\\nconfigurable_alternatives(which: ConfigurableField, *, default_key: str = \\'default\\', prefix_keys: bool = False, **kwargs: Runnable[Input, Output] | Callable[[], Runnable[Input, Output]]) ‚Üí RunnableSerializable#\\nConfigure alternatives for Runnables that can be set at runtime.\\n\\nParameters:\\n\\nwhich (ConfigurableField) ‚Äì The ConfigurableField instance that will be used to select the\\nalternative.\\ndefault_key (str) ‚Äì The default key to use if no alternative is selected.\\nDefaults to ‚Äúdefault‚Äù.\\nprefix_keys (bool) ‚Äì Whether to prefix the keys with the ConfigurableField id.\\nDefaults to False.\\n**kwargs (Runnable[Input, Output] | Callable[[], Runnable[Input, Output]]) ‚Äì A dictionary of keys to Runnable instances or callables that\\nreturn Runnable instances.\\n\\n\\nReturns:\\nA new Runnable with the alternatives configured.\\n\\nReturn type:\\nRunnableSerializable\\n\\n\\nfrom langchain_anthropic import ChatAnthropic\\nfrom langchain_core.runnables.utils import ConfigurableField\\nfrom langchain_openai import ChatOpenAI\\n\\nmodel = ChatAnthropic(\\n    model_name=\"claude-3-sonnet-20240229\"\\n).configurable_alternatives(\\n    ConfigurableField(id=\"llm\"),\\n    default_key=\"anthropic\",\\n    openai=ChatOpenAI()\\n)\\n\\n# uses the default model ChatAnthropic\\nprint(model.invoke(\"which organization created you?\").content)\\n\\n# uses ChatOpenAI\\nprint(\\n    model.with_config(\\n        configurable={\"llm\": \"openai\"}\\n    ).invoke(\"which organization created you?\").content\\n)\\n\\n\\n\\n\\n\\nconfigurable_fields(**kwargs: ConfigurableField | ConfigurableFieldSingleOption | ConfigurableFieldMultiOption) ‚Üí RunnableSerializable#\\nConfigure particular Runnable fields at runtime.\\n\\nParameters:\\n**kwargs (ConfigurableField | ConfigurableFieldSingleOption | ConfigurableFieldMultiOption) ‚Äì A dictionary of ConfigurableField instances to configure.\\n\\nReturns:\\nA new Runnable with the fields configured.\\n\\nReturn type:\\nRunnableSerializable\\n\\n\\nfrom langchain_core.runnables import ConfigurableField\\nfrom langchain_openai import ChatOpenAI\\n\\nmodel = ChatOpenAI(max_tokens=20).configurable_fields(\\n    max_tokens=ConfigurableField(\\n        id=\"output_token_number\",\\n        name=\"Max tokens in the output\",\\n        description=\"The maximum number of tokens in the output\",\\n    )\\n)\\n\\n# max_tokens = 20\\nprint(\\n    \"max_tokens_20: \",\\n    model.invoke(\"tell me something about chess\").content\\n)\\n\\n# max_tokens = 200\\nprint(\"max_tokens_200: \", model.with_config(\\n    configurable={\"output_token_number\": 200}\\n    ).invoke(\"tell me something about chess\").content\\n)\\n\\n\\n\\n\\n\\nextend(messages: Sequence[BaseMessagePromptTemplate | BaseMessage | BaseChatPromptTemplate | tuple[str | type, str | list[dict] | list[object]] | str]) ‚Üí None[source]#\\nExtend the chat template with a sequence of messages.\\n\\nParameters:\\nmessages (Sequence[BaseMessagePromptTemplate | BaseMessage | BaseChatPromptTemplate | tuple[str | type, str | list[dict] | list[object]] | str]) ‚Äì sequence of message representations to append.\\n\\nReturn type:\\nNone\\n\\n\\n\\n\\n\\nformat(**kwargs: Any) ‚Üí str#\\nFormat the chat template into a string.\\n\\nParameters:\\n**kwargs (Any) ‚Äì keyword arguments to use for filling in template variables\\nin all the template messages in this chat template.\\n\\nReturns:\\nformatted string.\\n\\nReturn type:\\nstr\\n\\n\\n\\n\\n\\nformat_messages(**kwargs: Any) ‚Üí list[BaseMessage][source]#\\nFormat the chat template into a list of finalized messages.\\n\\nParameters:\\n**kwargs (Any) ‚Äì keyword arguments to use for filling in template variables\\nin all the template messages in this chat template.\\n\\nReturns:\\nlist of formatted messages.\\n\\nReturn type:\\nlist[BaseMessage]\\n\\n\\n\\n\\n\\nformat_prompt(**kwargs: Any) ‚Üí PromptValue#\\nFormat prompt. Should return a PromptValue.\\n\\nParameters:\\n**kwargs (Any) ‚Äì Keyword arguments to use for formatting.\\n\\nReturns:\\nPromptValue.\\n\\nReturn type:\\nPromptValue\\n\\n\\n\\n\\n\\nclassmethod from_messages(messages: Sequence[BaseMessagePromptTemplate | BaseMessage | BaseChatPromptTemplate | tuple[str | type, str | list[dict] | list[object]] | str], template_format: Literal[\\'f-string\\', \\'mustache\\', \\'jinja2\\'] = \\'f-string\\') ‚Üí ChatPromptTemplate[source]#\\nCreate a chat prompt template from a variety of message formats.\\nExamples\\nInstantiation from a list of message templates:\\ntemplate = ChatPromptTemplate.from_messages([\\n    (\"human\", \"Hello, how are you?\"),\\n    (\"ai\", \"I\\'m doing well, thanks!\"),\\n    (\"human\", \"That\\'s good to hear.\"),\\n])\\n\\n\\nInstantiation from mixed message formats:\\ntemplate = ChatPromptTemplate.from_messages([\\n    SystemMessage(content=\"hello\"),\\n    (\"human\", \"Hello, how are you?\"),\\n])\\n\\n\\n\\nParameters:\\n\\nmessages (Sequence[BaseMessagePromptTemplate | BaseMessage | BaseChatPromptTemplate | tuple[str | type, str | list[dict] | list[object]] | str]) ‚Äì sequence of message representations.\\nA message can be represented using the following formats:\\n(1) BaseMessagePromptTemplate, (2) BaseMessage, (3) 2-tuple of\\n(message type, template); e.g., (‚Äúhuman‚Äù, ‚Äú{user_input}‚Äù),\\n(4) 2-tuple of (message class, template), (5) a string which is\\nshorthand for (‚Äúhuman‚Äù, template); e.g., ‚Äú{user_input}‚Äù.\\ntemplate_format (Literal[\\'f-string\\', \\'mustache\\', \\'jinja2\\']) ‚Äì format of the template. Defaults to ‚Äúf-string‚Äù.\\n\\n\\nReturns:\\na chat prompt template.\\n\\nReturn type:\\nChatPromptTemplate\\n\\n\\n\\n\\n\\nclassmethod from_role_strings(string_messages: list[tuple[str, str]]) ‚Üí ChatPromptTemplate[source]#\\n\\nDeprecated since version 0.0.1: Use from_messages classmethod() instead.\\n\\nCreate a chat prompt template from a list of (role, template) tuples.\\n\\nParameters:\\nstring_messages (list[tuple[str, str]]) ‚Äì list of (role, template) tuples.\\n\\nReturns:\\na chat prompt template.\\n\\nReturn type:\\nChatPromptTemplate\\n\\n\\n\\n\\n\\nclassmethod from_strings(string_messages: list[tuple[type[BaseMessagePromptTemplate], str]]) ‚Üí ChatPromptTemplate[source]#\\n\\nDeprecated since version 0.0.1: Use from_messages classmethod() instead.\\n\\nCreate a chat prompt template from a list of (role class, template) tuples.\\n\\nParameters:\\nstring_messages (list[tuple[type[BaseMessagePromptTemplate], str]]) ‚Äì list of (role class, template) tuples.\\n\\nReturns:\\na chat prompt template.\\n\\nReturn type:\\nChatPromptTemplate\\n\\n\\n\\n\\n\\nclassmethod from_template(template: str, **kwargs: Any) ‚Üí ChatPromptTemplate[source]#\\nCreate a chat prompt template from a template string.\\nCreates a chat template consisting of a single message assumed to be from\\nthe human.\\n\\nParameters:\\n\\ntemplate (str) ‚Äì template string\\n**kwargs (Any) ‚Äì keyword arguments to pass to the constructor.\\n\\n\\nReturns:\\nA new instance of this class.\\n\\nReturn type:\\nChatPromptTemplate\\n\\n\\n\\n\\n\\ninvoke(input: dict, config: RunnableConfig | None = None, **kwargs: Any) ‚Üí PromptValue#\\nInvoke the prompt.\\n\\nParameters:\\n\\ninput (dict) ‚Äì Dict, input to the prompt.\\nconfig (RunnableConfig | None) ‚Äì RunnableConfig, configuration for the prompt.\\nkwargs (Any)\\n\\n\\nReturns:\\nThe output of the prompt.\\n\\nReturn type:\\nPromptValue\\n\\n\\n\\n\\n\\npartial(**kwargs: Any) ‚Üí ChatPromptTemplate[source]#\\nGet a new ChatPromptTemplate with some input variables already filled in.\\n\\nParameters:\\n**kwargs (Any) ‚Äì keyword arguments to use for filling in template variables. Ought\\nto be a subset of the input variables.\\n\\nReturns:\\nA new ChatPromptTemplate.\\n\\nReturn type:\\nChatPromptTemplate\\n\\n\\nExample\\nfrom langchain_core.prompts import ChatPromptTemplate\\n\\ntemplate = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", \"You are an AI assistant named {name}.\"),\\n        (\"human\", \"Hi I\\'m {user}\"),\\n        (\"ai\", \"Hi there, {user}, I\\'m {name}.\"),\\n        (\"human\", \"{input}\"),\\n    ]\\n)\\ntemplate2 = template.partial(user=\"Lucy\", name=\"R2D2\")\\n\\ntemplate2.format_messages(input=\"hello\")\\n\\n\\n\\n\\n\\npretty_print() ‚Üí None#\\nPrint a human-readable representation.\\n\\nReturn type:\\nNone\\n\\n\\n\\n\\n\\npretty_repr(html: bool = False) ‚Üí str[source]#\\nHuman-readable representation.\\n\\nParameters:\\nhtml (bool) ‚Äì Whether to format as HTML. Defaults to False.\\n\\nReturns:\\nHuman-readable representation.\\n\\nReturn type:\\nstr\\n\\n\\n\\n\\n\\nsave(file_path: Path | str) ‚Üí None[source]#\\nSave prompt to file.\\n\\nParameters:\\nfile_path (Path | str) ‚Äì path to file.\\n\\nReturn type:\\nNone\\n\\n\\n\\n\\n\\nstream(input: Input, config: RunnableConfig | None = None, **kwargs: Any | None) ‚Üí Iterator[Output]#\\nDefault implementation of stream, which calls invoke.\\nSubclasses should override this method if they support streaming output.\\n\\nParameters:\\n\\ninput (Input) ‚Äì The input to the Runnable.\\nconfig (RunnableConfig | None) ‚Äì The config to use for the Runnable. Defaults to None.\\nkwargs (Any | None) ‚Äì Additional keyword arguments to pass to the Runnable.\\n\\n\\nYields:\\nThe output of the Runnable.\\n\\nReturn type:\\nIterator[Output]\\n\\n\\n\\n\\n\\nwith_alisteners(*, on_start: AsyncListener | None = None, on_end: AsyncListener | None = None, on_error: AsyncListener | None = None) ‚Üí Runnable[Input, Output]#\\nBind async lifecycle listeners to a Runnable, returning a new Runnable.\\non_start: Asynchronously called before the Runnable starts running.\\non_end: Asynchronously called after the Runnable finishes running.\\non_error: Asynchronously called if the Runnable throws an error.\\nThe Run object contains information about the run, including its id,\\ntype, input, output, error, start_time, end_time, and any tags or metadata\\nadded to the run.\\n\\nParameters:\\n\\non_start (Optional[AsyncListener]) ‚Äì Asynchronously called before the Runnable starts running.\\nDefaults to None.\\non_end (Optional[AsyncListener]) ‚Äì Asynchronously called after the Runnable finishes running.\\nDefaults to None.\\non_error (Optional[AsyncListener]) ‚Äì Asynchronously called if the Runnable throws an error.\\nDefaults to None.\\n\\n\\nReturns:\\nA new Runnable with the listeners bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\nimport time\\n\\nasync def test_runnable(time_to_sleep : int):\\n    print(f\"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}\")\\n    await asyncio.sleep(time_to_sleep)\\n    print(f\"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}\")\\n\\nasync def fn_start(run_obj : Runnable):\\n    print(f\"on start callback starts at {format_t(time.time())}\\n    await asyncio.sleep(3)\\n    print(f\"on start callback ends at {format_t(time.time())}\")\\n\\nasync def fn_end(run_obj : Runnable):\\n    print(f\"on end callback starts at {format_t(time.time())}\\n    await asyncio.sleep(2)\\n    print(f\"on end callback ends at {format_t(time.time())}\")\\n\\nrunnable = RunnableLambda(test_runnable).with_alisteners(\\n    on_start=fn_start,\\n    on_end=fn_end\\n)\\nasync def concurrent_runs():\\n    await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))\\n\\nasyncio.run(concurrent_runs())\\nResult:\\non start callback starts at 2024-05-16T14:20:29.637053+00:00\\non start callback starts at 2024-05-16T14:20:29.637150+00:00\\non start callback ends at 2024-05-16T14:20:32.638305+00:00\\non start callback ends at 2024-05-16T14:20:32.638383+00:00\\nRunnable[3s]: starts at 2024-05-16T14:20:32.638849+00:00\\nRunnable[5s]: starts at 2024-05-16T14:20:32.638999+00:00\\nRunnable[3s]: ends at 2024-05-16T14:20:35.640016+00:00\\non end callback starts at 2024-05-16T14:20:35.640534+00:00\\nRunnable[5s]: ends at 2024-05-16T14:20:37.640169+00:00\\non end callback starts at 2024-05-16T14:20:37.640574+00:00\\non end callback ends at 2024-05-16T14:20:37.640654+00:00\\non end callback ends at 2024-05-16T14:20:39.641751+00:00\\n\\n\\n\\n\\n\\nwith_config(config: RunnableConfig | None = None, **kwargs: Any) ‚Üí Runnable[Input, Output]#\\nBind config to a Runnable, returning a new Runnable.\\n\\nParameters:\\n\\nconfig (RunnableConfig | None) ‚Äì The config to bind to the Runnable.\\nkwargs (Any) ‚Äì Additional keyword arguments to pass to the Runnable.\\n\\n\\nReturns:\\nA new Runnable with the config bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\n\\n\\n\\nwith_fallbacks(fallbacks: Sequence[Runnable[Input, Output]], *, exceptions_to_handle: tuple[type[BaseException], ...] = (<class \\'Exception\\'>,), exception_key: Optional[str] = None) ‚Üí RunnableWithFallbacksT[Input, Output]#\\nAdd fallbacks to a Runnable, returning a new Runnable.\\nThe new Runnable will try the original Runnable, and then each fallback\\nin order, upon failures.\\n\\nParameters:\\n\\nfallbacks (Sequence[Runnable[Input, Output]]) ‚Äì A sequence of runnables to try if the original Runnable fails.\\nexceptions_to_handle (tuple[type[BaseException], ...]) ‚Äì A tuple of exception types to handle.\\nDefaults to (Exception,).\\nexception_key (Optional[str]) ‚Äì If string is specified then handled exceptions will be passed\\nto fallbacks as part of the input under the specified key. If None,\\nexceptions will not be passed to fallbacks. If used, the base Runnable\\nand its fallbacks must accept a dictionary as input. Defaults to None.\\n\\n\\nReturns:\\nA new Runnable that will try the original Runnable, and then each\\nfallback in order, upon failures.\\n\\nReturn type:\\nRunnableWithFallbacksT[Input, Output]\\n\\n\\nExample\\nfrom typing import Iterator\\n\\nfrom langchain_core.runnables import RunnableGenerator\\n\\n\\ndef _generate_immediate_error(input: Iterator) -> Iterator[str]:\\n    raise ValueError()\\n    yield \"\"\\n\\n\\ndef _generate(input: Iterator) -> Iterator[str]:\\n    yield from \"foo bar\"\\n\\n\\nrunnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(\\n    [RunnableGenerator(_generate)]\\n    )\\nprint(\\'\\'.join(runnable.stream({}))) #foo bar\\n\\n\\n\\nParameters:\\n\\nfallbacks (Sequence[Runnable[Input, Output]]) ‚Äì A sequence of runnables to try if the original Runnable fails.\\nexceptions_to_handle (tuple[type[BaseException], ...]) ‚Äì A tuple of exception types to handle.\\nexception_key (Optional[str]) ‚Äì If string is specified then handled exceptions will be passed\\nto fallbacks as part of the input under the specified key. If None,\\nexceptions will not be passed to fallbacks. If used, the base Runnable\\nand its fallbacks must accept a dictionary as input.\\n\\n\\nReturns:\\nA new Runnable that will try the original Runnable, and then each\\nfallback in order, upon failures.\\n\\nReturn type:\\nRunnableWithFallbacksT[Input, Output]\\n\\n\\n\\n\\n\\nwith_listeners(*, on_start: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None, on_end: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None, on_error: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None) ‚Üí Runnable[Input, Output]#\\nBind lifecycle listeners to a Runnable, returning a new Runnable.\\non_start: Called before the Runnable starts running, with the Run object.\\non_end: Called after the Runnable finishes running, with the Run object.\\non_error: Called if the Runnable throws an error, with the Run object.\\nThe Run object contains information about the run, including its id,\\ntype, input, output, error, start_time, end_time, and any tags or metadata\\nadded to the run.\\n\\nParameters:\\n\\non_start (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) ‚Äì Called before the Runnable starts running. Defaults to None.\\non_end (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) ‚Äì Called after the Runnable finishes running. Defaults to None.\\non_error (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) ‚Äì Called if the Runnable throws an error. Defaults to None.\\n\\n\\nReturns:\\nA new Runnable with the listeners bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\nfrom langchain_core.tracers.schemas import Run\\n\\nimport time\\n\\ndef test_runnable(time_to_sleep : int):\\n    time.sleep(time_to_sleep)\\n\\ndef fn_start(run_obj: Run):\\n    print(\"start_time:\", run_obj.start_time)\\n\\ndef fn_end(run_obj: Run):\\n    print(\"end_time:\", run_obj.end_time)\\n\\nchain = RunnableLambda(test_runnable).with_listeners(\\n    on_start=fn_start,\\n    on_end=fn_end\\n)\\nchain.invoke(2)\\n\\n\\n\\n\\n\\nwith_retry(*, retry_if_exception_type: tuple[type[BaseException], ...] = (<class \\'Exception\\'>,), wait_exponential_jitter: bool = True, stop_after_attempt: int = 3) ‚Üí Runnable[Input, Output]#\\nCreate a new Runnable that retries the original Runnable on exceptions.\\n\\nParameters:\\n\\nretry_if_exception_type (tuple[type[BaseException], ...]) ‚Äì A tuple of exception types to retry on.\\nDefaults to (Exception,).\\nwait_exponential_jitter (bool) ‚Äì Whether to add jitter to the wait\\ntime between retries. Defaults to True.\\nstop_after_attempt (int) ‚Äì The maximum number of attempts to make before\\ngiving up. Defaults to 3.\\n\\n\\nReturns:\\nA new Runnable that retries the original Runnable on exceptions.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\n\\ncount = 0\\n\\n\\ndef _lambda(x: int) -> None:\\n    global count\\n    count = count + 1\\n    if x == 1:\\n        raise ValueError(\"x is 1\")\\n    else:\\n         pass\\n\\n\\nrunnable = RunnableLambda(_lambda)\\ntry:\\n    runnable.with_retry(\\n        stop_after_attempt=2,\\n        retry_if_exception_type=(ValueError,),\\n    ).invoke(1)\\nexcept ValueError:\\n    pass\\n\\nassert (count == 2)\\n\\n\\n\\nParameters:\\n\\nretry_if_exception_type (tuple[type[BaseException], ...]) ‚Äì A tuple of exception types to retry on\\nwait_exponential_jitter (bool) ‚Äì Whether to add jitter to the wait time\\nbetween retries\\nstop_after_attempt (int) ‚Äì The maximum number of attempts to make before giving up\\n\\n\\nReturns:\\nA new Runnable that retries the original Runnable on exceptions.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\n\\n\\n\\nwith_types(*, input_type: type[Input] | None = None, output_type: type[Output] | None = None) ‚Üí Runnable[Input, Output]#\\nBind input and output types to a Runnable, returning a new Runnable.\\n\\nParameters:\\n\\ninput_type (type[Input] | None) ‚Äì The input type to bind to the Runnable. Defaults to None.\\noutput_type (type[Output] | None) ‚Äì The output type to bind to the Runnable. Defaults to None.\\n\\n\\nReturns:\\nA new Runnable with the types bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\n\\n\\nExamples using ChatPromptTemplate\\n\\n# Basic example (short documents)\\n# Example\\n# Legacy\\nAWS DynamoDB\\nActiveloop Deep Memory\\nApache Cassandra\\nApertureDB\\nArxivRetriever\\nAskNews\\nAzureAISearchRetriever\\nAzureChatOpenAI\\nBuild a Chatbot\\nBuild a Local RAG Application\\nBuild a PDF ingestion and Question/Answering system\\nBuild a Query Analysis System\\nBuild a Retrieval Augmented Generation (RAG) App\\nBuild a Simple LLM Application with LCEL\\nBuild an Extraction Chain\\nChatAI21\\nChatAnthropic\\nChatBedrock\\nChatDatabricks\\nChatFireworks\\nChatGoogleGenerativeAI\\nChatGroq\\nChatMistralAI\\nChatNVIDIA\\nChatOCIGenAI\\nChatOllama\\nChatOpenAI\\nChatPerplexity\\nChatTogether\\nChatUpstage\\nChatVertexAI\\nChatWatsonx\\nChatYI\\nClassify Text into Labels\\nCohere\\nConceptual guide\\nContext\\nConversational RAG\\nCouchbase\\nDatabricks Unity Catalog (UC)\\nEden AI\\nElasticsearchRetriever\\nFacebook Messenger\\nFiddler\\nFigma\\nFinancialDatasets Toolkit\\nFleet AI Context\\nGoogle AlloyDB for PostgreSQL\\nGoogle El Carro Oracle\\nGoogle SQL for MySQL\\nGoogle SQL for PostgreSQL\\nGoogle SQL for SQL Server\\nGoogle Vertex AI Search\\nHow deal with high cardinality categoricals when doing query analysis\\nHow to add a semantic layer over graph database\\nHow to add ad-hoc tool calling capability to LLMs and Chat Models\\nHow to add chat history\\nHow to add default invocation args to a Runnable\\nHow to add examples to the prompt for query analysis\\nHow to add fallbacks to a runnable\\nHow to add memory to chatbots\\nHow to add message history\\nHow to add retrieval to chatbots\\nHow to add tools to chatbots\\nHow to add values to a chain‚Äôs state\\nHow to attach callbacks to a runnable\\nHow to chain runnables\\nHow to convert Runnables as Tools\\nHow to create a custom LLM class\\nHow to create a dynamic (self-constructing) chain\\nHow to create custom callback handlers\\nHow to create tools\\nHow to deal with large databases when doing SQL question-answering\\nHow to debug your LLM apps\\nHow to do per-user retrieval\\nHow to do query validation as part of SQL question-answering\\nHow to do question answering over CSVs\\nHow to do tool/function calling\\nHow to get a RAG application to add citations\\nHow to get your RAG application to return sources\\nHow to handle cases where no queries are generated\\nHow to handle long text when doing extraction\\nHow to handle multiple queries when doing query analysis\\nHow to handle multiple retrievers when doing query analysis\\nHow to handle tool errors\\nHow to inspect runnables\\nHow to invoke runnables in parallel\\nHow to map values to a graph database\\nHow to migrate from legacy LangChain agents to LangGraph\\nHow to pass callbacks in at runtime\\nHow to pass through arguments from one step to the next\\nHow to propagate callbacks  constructor\\nHow to retrieve using multiple vectors per document\\nHow to return structured data from a model\\nHow to run custom functions\\nHow to save and load LangChain objects\\nHow to stream events from a tool\\nHow to stream results from your RAG application\\nHow to stream runnables\\nHow to summarize text in a single LLM call\\nHow to summarize text through iterative refinement\\nHow to summarize text through parallelization\\nHow to track token usage in ChatModels\\nHow to use few shot examples in chat models\\nHow to use few-shot prompting with tool calling\\nHow to use multimodal prompts\\nHow to use prompting alone (no tool calling) to do extraction\\nHow to use reference examples when doing extraction\\nHybrid Search\\nImage captions\\nJaguar Vector Database\\nJinaChat\\nKinetica Language To SQL Chat Model\\nLangChain Expression Language Cheatsheet\\nLangSmith LLM Runs\\nLlama.cpp\\nLlama2Chat\\nLoad docs\\nMLflow\\nMaritalk\\nMongoDB\\nNVIDIA NIMs \\nOllamaFunctions\\nOllamaLLM\\nOpenAI metadata tagger\\nRAGatouille\\nRedis\\nRiza Code Interpreter\\nSQL (SQLAlchemy)\\nSQLite\\nStreamlit\\nSummarize Text\\nTavily Search\\nTavilySearchAPIRetriever\\nTiDB\\nUpTrain\\nVector stores and retrievers\\nWeaviate\\nWikipediaRetriever\\nYellowbrick\\nYou.com\\nYuan2.0\\nZepCloudChatMessageHistory\\niMessage\\nvLLM Chat\\nü¶úÔ∏èüèì LangServe\\n\\n\\n\\n\\n\\n\\n\\n\\n On this page\\n  \\n\\n\\nChatPromptTemplate\\ninput_types\\ninput_variables\\nmessages\\nmetadata\\noptional_variables\\noutput_parser\\npartial_variables\\ntags\\nvalidate_template\\nabatch()\\nabatch_as_completed()\\naformat()\\naformat_messages()\\naformat_prompt()\\nainvoke()\\nappend()\\nastream()\\nastream_events()\\nbatch()\\nbatch_as_completed()\\nbind()\\nconfigurable_alternatives()\\nconfigurable_fields()\\nextend()\\nformat()\\nformat_messages()\\nformat_prompt()\\nfrom_messages()\\nfrom_role_strings()\\nfrom_strings()\\nfrom_template()\\ninvoke()\\npartial()\\npretty_print()\\npretty_repr()\\nsave()\\nstream()\\nwith_alisteners()\\nwith_config()\\nwith_fallbacks()\\nwith_listeners()\\nwith_retry()\\nwith_types()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n      ¬© Copyright 2023, LangChain Inc.\\n      \\n\\n\\n\\n\\n\\n\\n')]\n"
     ]
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data -->then divide to chunks--->embedding to vector from text-->then store in vector embedding\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000,chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main content\\n\\n\\nBack to top\\n\\n\\n\\n\\nCtrl+K\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Reference\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCtrl+K\\n\\n\\n\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\nX / Twitter\\n\\n\\n\\n\\n\\n\\n\\n\\nCtrl+K\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Reference\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\nX / Twitter\\n\\n\\n\\n\\n\\n\\n\\nSection Navigation\\nBase packages'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='Ctrl+K\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Reference\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\nX / Twitter\\n\\n\\n\\n\\n\\n\\n\\nSection Navigation\\nBase packages\\n\\nCore\\nagents\\nbeta\\ncaches\\ncallbacks\\nchat_history\\nchat_loaders\\nchat_sessions\\ndocument_loaders\\ndocuments\\nembeddings\\nexample_selectors\\nexceptions\\nglobals\\nindexing\\nlanguage_models\\nload\\nmessages\\noutput_parsers\\noutputs\\nprompt_values\\nprompts\\nBasePromptTemplate\\nAIMessagePromptTemplate\\nBaseChatPromptTemplate\\nBaseMessagePromptTemplate\\nBaseStringMessagePromptTemplate\\nChatMessagePromptTemplate\\nChatPromptTemplate\\nHumanMessagePromptTemplate\\nMessagesPlaceholder\\nSystemMessagePromptTemplate\\nFewShotChatMessagePromptTemplate\\nFewShotPromptTemplate\\nFewShotPromptWithTemplates\\nImagePromptTemplate\\nPromptTemplate\\nStringPromptTemplate\\nStructuredPrompt\\naformat_document\\nformat_document\\nload_prompt\\nload_prompt_from_config\\ncheck_valid_template\\nget_template_variables\\njinja2_formatter\\nmustache_formatter\\nmustache_schema\\nmustache_template_vars\\nvalidate_jinja2\\nPipelinePromptTemplate'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='rate_limiters\\nretrievers\\nrunnables\\nstores\\nstructured_query\\nsys_info\\ntools\\ntracers\\nutils\\nvectorstores\\n\\n\\nLangchain\\nText Splitters\\nCommunity\\nExperimental\\n\\nIntegrations\\n\\nAI21\\nAnthropic\\nAstraDB\\nAWS\\nAzure Dynamic Sessions\\nCerebras\\nChroma\\nCohere\\nDatabricks\\nDeepseek\\nElasticsearch\\nExa\\nFireworks\\nGoogle Community\\nGoogle GenAI\\nGoogle VertexAI\\nGroq\\nHuggingface\\nIBM\\nMilvus\\nMistralAI\\nNeo4J\\nNomic\\nNvidia Ai Endpoints\\nOllama\\nOpenAI\\nPinecone\\nPostgres\\nPrompty\\nQdrant\\nRedis\\nSema4\\nSnowflake\\nSqlserver\\nStandard Tests\\nTogether\\nUnstructured\\nUpstage\\nVoyageAI\\nWeaviate\\nXAI\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Python API Reference\\nlangchain-core: 0.3.33\\nprompts\\nChatPromptTemplate\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nChatPromptTemplate#\\n\\n\\nclass langchain_core.prompts.chat.ChatPromptTemplate[source]#\\nBases: BaseChatPromptTemplate\\nPrompt template for chat models.\\nUse to create flexible templated prompts for chat models.\\nExamples'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='class langchain_core.prompts.chat.ChatPromptTemplate[source]#\\nBases: BaseChatPromptTemplate\\nPrompt template for chat models.\\nUse to create flexible templated prompts for chat models.\\nExamples\\n\\nChanged in version 0.2.24: You can pass any Message-like formats supported by\\nChatPromptTemplate.from_messages() directly to ChatPromptTemplate()\\ninit.\\n\\nfrom langchain_core.prompts import ChatPromptTemplate\\n\\ntemplate = ChatPromptTemplate([\\n    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\\n    (\"human\", \"Hello, how are you doing?\"),\\n    (\"ai\", \"I\\'m doing well, thanks!\"),\\n    (\"human\", \"{user_input}\"),\\n])'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='prompt_value = template.invoke(\\n    {\\n        \"name\": \"Bob\",\\n        \"user_input\": \"What is your name?\"\\n    }\\n)\\n# Output:\\n# ChatPromptValue(\\n#    messages=[\\n#        SystemMessage(content=\\'You are a helpful AI bot. Your name is Bob.\\'),\\n#        HumanMessage(content=\\'Hello, how are you doing?\\'),\\n#        AIMessage(content=\"I\\'m doing well, thanks!\"),\\n#        HumanMessage(content=\\'What is your name?\\')\\n#    ]\\n#)\\n\\n\\nMessages Placeholder:\\n\\n# In addition to Human/AI/Tool/Function messages,\\n# you can initialize the template with a MessagesPlaceholder\\n# either using the class directly or with the shorthand tuple syntax:\\n\\ntemplate = ChatPromptTemplate([\\n    (\"system\", \"You are a helpful AI bot.\"),\\n    # Means the template will receive an optional list of messages under\\n    # the \"conversation\" key\\n    (\"placeholder\", \"{conversation}\")\\n    # Equivalently:\\n    # MessagesPlaceholder(variable_name=\"conversation\", optional=True)\\n])'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='prompt_value = template.invoke(\\n    {\\n        \"conversation\": [\\n            (\"human\", \"Hi!\"),\\n            (\"ai\", \"How can I assist you today?\"),\\n            (\"human\", \"Can you make me an ice cream sundae?\"),\\n            (\"ai\", \"No.\")\\n        ]\\n    }\\n)\\n\\n# Output:\\n# ChatPromptValue(\\n#    messages=[\\n#        SystemMessage(content=\\'You are a helpful AI bot.\\'),\\n#        HumanMessage(content=\\'Hi!\\'),\\n#        AIMessage(content=\\'How can I assist you today?\\'),\\n#        HumanMessage(content=\\'Can you make me an ice cream sundae?\\'),\\n#        AIMessage(content=\\'No.\\'),\\n#    ]\\n#)\\n\\n\\n\\nSingle-variable template:\\n\\nIf your prompt has only a single input variable (i.e., 1 instance of ‚Äú{variable_nams}‚Äù),\\nand you invoke the template with a non-dict object, the prompt template will\\ninject the provided argument into that variable location.\\nfrom langchain_core.prompts import ChatPromptTemplate'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='template = ChatPromptTemplate([\\n    (\"system\", \"You are a helpful AI bot. Your name is Carl.\"),\\n    (\"human\", \"{user_input}\"),\\n])\\n\\nprompt_value = template.invoke(\"Hello, there!\")\\n# Equivalent to\\n# prompt_value = template.invoke({\"user_input\": \"Hello, there!\"})\\n\\n# Output:\\n#  ChatPromptValue(\\n#     messages=[\\n#         SystemMessage(content=\\'You are a helpful AI bot. Your name is Carl.\\'),\\n#         HumanMessage(content=\\'Hello, there!\\'),\\n#     ]\\n# )\\n\\n\\n\\nCreate a chat prompt template from a variety of message formats.\\n\\nParameters:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='messages ‚Äì sequence of message representations.\\nA message can be represented using the following formats:\\n(1) BaseMessagePromptTemplate, (2) BaseMessage, (3) 2-tuple of\\n(message type, template); e.g., (‚Äúhuman‚Äù, ‚Äú{user_input}‚Äù),\\n(4) 2-tuple of (message class, template), (5) a string which is\\nshorthand for (‚Äúhuman‚Äù, template); e.g., ‚Äú{user_input}‚Äù.\\ntemplate_format ‚Äì format of the template. Defaults to ‚Äúf-string‚Äù.\\ninput_variables ‚Äì A list of the names of the variables whose values are\\nrequired as inputs to the prompt.\\noptional_variables ‚Äì A list of the names of the variables for placeholder\\ninferred (or MessagePlaceholder that are optional. These variables are auto)\\nthem. (from the prompt and user need not provide)\\npartial_variables ‚Äì A dictionary of the partial variables the prompt\\ntemplate carries. Partial variables populate the template so that you\\ndon‚Äôt need to pass them in every time you call the prompt.\\nvalidate_template ‚Äì Whether to validate the template.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='template carries. Partial variables populate the template so that you\\ndon‚Äôt need to pass them in every time you call the prompt.\\nvalidate_template ‚Äì Whether to validate the template.\\ninput_types ‚Äì A dictionary of the types of the variables the prompt template\\nexpects. If not provided, all variables are assumed to be strings.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='Returns:\\nA chat prompt template.\\n\\n\\nExamples\\nInstantiation from a list of message templates:\\ntemplate = ChatPromptTemplate([\\n    (\"human\", \"Hello, how are you?\"),\\n    (\"ai\", \"I\\'m doing well, thanks!\"),\\n    (\"human\", \"That\\'s good to hear.\"),\\n])\\n\\n\\nInstantiation from mixed message formats:\\ntemplate = ChatPromptTemplate([\\n    SystemMessage(content=\"hello\"),\\n    (\"human\", \"Hello, how are you?\"),\\n])\\n\\n\\n\\nNote\\nChatPromptTemplate implements the standard Runnable Interface. üèÉ\\nThe Runnable Interface has additional methods that are available on runnables, such as with_types, with_retry, assign, bind, get_graph, and more.\\n\\n\\n\\nparam input_types: Dict[str, Any] [Optional]#\\nA dictionary of the types of the variables the prompt template expects.\\nIf not provided, all variables are assumed to be strings.\\n\\n\\n\\nparam input_variables: list[str] [Required]#\\nA list of the names of the variables whose values are required as inputs to the\\nprompt.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='param input_variables: list[str] [Required]#\\nA list of the names of the variables whose values are required as inputs to the\\nprompt.\\n\\n\\n\\nparam messages: Annotated[list[MessageLike], SkipValidation()] [Required]#\\nList of messages consisting of either message prompt templates or messages.\\n\\n\\n\\nparam metadata: Dict[str, Any] | None = None#\\nMetadata to be used for tracing.\\n\\n\\n\\nparam optional_variables: list[str] = []#\\noptional_variables: A list of the names of the variables for placeholder\\nor MessagePlaceholder that are optional. These variables are auto inferred\\nfrom the prompt and user need not provide them.\\n\\n\\n\\nparam output_parser: BaseOutputParser | None = None#\\nHow to parse the output of calling an LLM on this formatted prompt.\\n\\n\\n\\nparam partial_variables: Mapping[str, Any] [Optional]#\\nA dictionary of the partial variables the prompt template carries.\\nPartial variables populate the template so that you don‚Äôt need to\\npass them in every time you call the prompt.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='param tags: list[str] | None = None#\\nTags to be used for tracing.\\n\\n\\n\\nparam validate_template: bool = False#\\nWhether or not to try validating the template.\\n\\n\\n\\nasync abatch(inputs: list[Input], config: RunnableConfig | list[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None) ‚Üí list[Output]#\\nDefault implementation runs ainvoke in parallel using asyncio.gather.\\nThe default implementation of batch works well for IO bound runnables.\\nSubclasses should override this method if they can batch more efficiently;\\ne.g., if the underlying Runnable uses an API which supports a batch mode.\\n\\nParameters:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='Parameters:\\n\\ninputs (list[Input]) ‚Äì A list of inputs to the Runnable.\\nconfig (RunnableConfig | list[RunnableConfig] | None) ‚Äì A config to use when invoking the Runnable.\\nThe config supports standard keys like ‚Äòtags‚Äô, ‚Äòmetadata‚Äô for tracing\\npurposes, ‚Äòmax_concurrency‚Äô for controlling how much work to do\\nin parallel, and other keys. Please refer to the RunnableConfig\\nfor more details. Defaults to None.\\nreturn_exceptions (bool) ‚Äì Whether to return exceptions instead of raising them.\\nDefaults to False.\\nkwargs (Any | None) ‚Äì Additional keyword arguments to pass to the Runnable.\\n\\n\\nReturns:\\nA list of outputs from the Runnable.\\n\\nReturn type:\\nlist[Output]\\n\\n\\n\\n\\n\\nasync abatch_as_completed(inputs: Sequence[Input], config: RunnableConfig | Sequence[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None) ‚Üí AsyncIterator[tuple[int, Output | Exception]]#\\nRun ainvoke in parallel on a list of inputs,\\nyielding results as they complete.\\n\\nParameters:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='Parameters:\\n\\ninputs (Sequence[Input]) ‚Äì A list of inputs to the Runnable.\\nconfig (RunnableConfig | Sequence[RunnableConfig] | None) ‚Äì A config to use when invoking the Runnable.\\nThe config supports standard keys like ‚Äòtags‚Äô, ‚Äòmetadata‚Äô for tracing\\npurposes, ‚Äòmax_concurrency‚Äô for controlling how much work to do\\nin parallel, and other keys. Please refer to the RunnableConfig\\nfor more details. Defaults to None. Defaults to None.\\nreturn_exceptions (bool) ‚Äì Whether to return exceptions instead of raising them.\\nDefaults to False.\\nkwargs (Any | None) ‚Äì Additional keyword arguments to pass to the Runnable.\\n\\n\\nYields:\\nA tuple of the index of the input and the output from the Runnable.\\n\\nReturn type:\\nAsyncIterator[tuple[int, Output | Exception]]\\n\\n\\n\\n\\n\\nasync aformat(**kwargs: Any) ‚Üí str#\\nAsync format the chat template into a string.\\n\\nParameters:\\n**kwargs (Any) ‚Äì keyword arguments to use for filling in template variables\\nin all the template messages in this chat template.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='Parameters:\\n**kwargs (Any) ‚Äì keyword arguments to use for filling in template variables\\nin all the template messages in this chat template.\\n\\nReturns:\\nformatted string.\\n\\nReturn type:\\nstr\\n\\n\\n\\n\\n\\nasync aformat_messages(**kwargs: Any) ‚Üí list[BaseMessage][source]#\\nAsync format the chat template into a list of finalized messages.\\n\\nParameters:\\n**kwargs (Any) ‚Äì keyword arguments to use for filling in template variables\\nin all the template messages in this chat template.\\n\\nReturns:\\nlist of formatted messages.\\n\\nRaises:\\nValueError ‚Äì If unexpected input.\\n\\nReturn type:\\nlist[BaseMessage]\\n\\n\\n\\n\\n\\nasync aformat_prompt(**kwargs: Any) ‚Üí PromptValue#\\nAsync format prompt. Should return a PromptValue.\\n\\nParameters:\\n**kwargs (Any) ‚Äì Keyword arguments to use for formatting.\\n\\nReturns:\\nPromptValue.\\n\\nReturn type:\\nPromptValue\\n\\n\\n\\n\\n\\nasync ainvoke(input: dict, config: RunnableConfig | None = None, **kwargs: Any) ‚Üí PromptValue#\\nAsync invoke the prompt.\\n\\nParameters:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='Returns:\\nPromptValue.\\n\\nReturn type:\\nPromptValue\\n\\n\\n\\n\\n\\nasync ainvoke(input: dict, config: RunnableConfig | None = None, **kwargs: Any) ‚Üí PromptValue#\\nAsync invoke the prompt.\\n\\nParameters:\\n\\ninput (dict) ‚Äì Dict, input to the prompt.\\nconfig (RunnableConfig | None) ‚Äì RunnableConfig, configuration for the prompt.\\nkwargs (Any)\\n\\n\\nReturns:\\nThe output of the prompt.\\n\\nReturn type:\\nPromptValue\\n\\n\\n\\n\\n\\nappend(message: BaseMessagePromptTemplate | BaseMessage | BaseChatPromptTemplate | tuple[str | type, str | list[dict] | list[object]] | str) ‚Üí None[source]#\\nAppend a message to the end of the chat template.\\n\\nParameters:\\nmessage (BaseMessagePromptTemplate | BaseMessage | BaseChatPromptTemplate | tuple[str | type, str | list[dict] | list[object]] | str) ‚Äì representation of a message to append.\\n\\nReturn type:\\nNone'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='Return type:\\nNone\\n\\n\\n\\n\\n\\nasync astream(input: Input, config: RunnableConfig | None = None, **kwargs: Any | None) ‚Üí AsyncIterator[Output]#\\nDefault implementation of astream, which calls ainvoke.\\nSubclasses should override this method if they support streaming output.\\n\\nParameters:\\n\\ninput (Input) ‚Äì The input to the Runnable.\\nconfig (RunnableConfig | None) ‚Äì The config to use for the Runnable. Defaults to None.\\nkwargs (Any | None) ‚Äì Additional keyword arguments to pass to the Runnable.\\n\\n\\nYields:\\nThe output of the Runnable.\\n\\nReturn type:\\nAsyncIterator[Output]'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content=\"Yields:\\nThe output of the Runnable.\\n\\nReturn type:\\nAsyncIterator[Output]\\n\\n\\n\\n\\n\\nasync astream_events(input: Any, config: RunnableConfig | None = None, *, version: Literal['v1', 'v2'], include_names: Sequence[str] | None = None, include_types: Sequence[str] | None = None, include_tags: Sequence[str] | None = None, exclude_names: Sequence[str] | None = None, exclude_types: Sequence[str] | None = None, exclude_tags: Sequence[str] | None = None, **kwargs: Any) ‚Üí AsyncIterator[StandardStreamEvent | CustomStreamEvent]#\\nGenerate a stream of events.\\nUse to create an iterator over StreamEvents that provide real-time information\\nabout the progress of the Runnable, including StreamEvents from intermediate\\nresults.\\nA StreamEvent is a dictionary with the following schema:\\n\\n\\nevent: str - Event names are of theformat: on_[runnable_type]_(start|stream|end).\\n\\n\\n\\nname: str - The name of the Runnable that generated the event.\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='event: str - Event names are of theformat: on_[runnable_type]_(start|stream|end).\\n\\n\\n\\nname: str - The name of the Runnable that generated the event.\\n\\nrun_id: str - randomly generated ID associated with the given execution ofthe Runnable that emitted the event.\\nA child Runnable that gets invoked as part of the execution of a\\nparent Runnable is assigned its own unique ID.\\n\\n\\n\\n\\nparent_ids: List[str] - The IDs of the parent runnables thatgenerated the event. The root Runnable will have an empty list.\\nThe order of the parent IDs is from the root to the immediate parent.\\nOnly available for v2 version of the API. The v1 version of the API\\nwill return an empty list.\\n\\n\\n\\n\\ntags: Optional[List[str]] - The tags of the Runnable that generatedthe event.\\n\\n\\n\\n\\nmetadata: Optional[Dict[str, Any]] - The metadata of the Runnablethat generated the event.\\n\\n\\n\\ndata: Dict[str, Any]'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='tags: Optional[List[str]] - The tags of the Runnable that generatedthe event.\\n\\n\\n\\n\\nmetadata: Optional[Dict[str, Any]] - The metadata of the Runnablethat generated the event.\\n\\n\\n\\ndata: Dict[str, Any]\\n\\nBelow is a table that illustrates some events that might be emitted by various\\nchains. Metadata fields have been omitted from the table for brevity.\\nChain definitions have been included after the table.\\nATTENTION This reference table is for the V2 version of the schema.\\n\\n\\nevent\\nname\\nchunk\\ninput\\noutput\\n\\n\\n\\non_chat_model_start\\n[model name]\\n\\n{‚Äúmessages‚Äù: [[SystemMessage, HumanMessage]]}\\n\\n\\non_chat_model_stream\\n[model name]\\nAIMessageChunk(content=‚Äùhello‚Äù)\\n\\n\\n\\non_chat_model_end\\n[model name]\\n\\n{‚Äúmessages‚Äù: [[SystemMessage, HumanMessage]]}\\nAIMessageChunk(content=‚Äùhello world‚Äù)\\n\\non_llm_start\\n[model name]\\n\\n{‚Äòinput‚Äô: ‚Äòhello‚Äô}\\n\\n\\non_llm_stream\\n[model name]\\n‚ÄòHello‚Äô\\n\\n\\n\\non_llm_end\\n[model name]\\n\\n‚ÄòHello human!‚Äô\\n\\n\\non_chain_start\\nformat_docs\\n\\n\\n\\n\\non_chain_stream\\nformat_docs\\n‚Äúhello world!, goodbye world!‚Äù'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='{‚Äòinput‚Äô: ‚Äòhello‚Äô}\\n\\n\\non_llm_stream\\n[model name]\\n‚ÄòHello‚Äô\\n\\n\\n\\non_llm_end\\n[model name]\\n\\n‚ÄòHello human!‚Äô\\n\\n\\non_chain_start\\nformat_docs\\n\\n\\n\\n\\non_chain_stream\\nformat_docs\\n‚Äúhello world!, goodbye world!‚Äù\\n\\n\\n\\non_chain_end\\nformat_docs\\n\\n[Document(‚Ä¶)]\\n‚Äúhello world!, goodbye world!‚Äù\\n\\non_tool_start\\nsome_tool\\n\\n{‚Äúx‚Äù: 1, ‚Äúy‚Äù: ‚Äú2‚Äù}\\n\\n\\non_tool_end\\nsome_tool\\n\\n\\n{‚Äúx‚Äù: 1, ‚Äúy‚Äù: ‚Äú2‚Äù}\\n\\non_retriever_start\\n[retriever name]\\n\\n{‚Äúquery‚Äù: ‚Äúhello‚Äù}\\n\\n\\non_retriever_end\\n[retriever name]\\n\\n{‚Äúquery‚Äù: ‚Äúhello‚Äù}\\n[Document(‚Ä¶), ..]\\n\\non_prompt_start\\n[template_name]\\n\\n{‚Äúquestion‚Äù: ‚Äúhello‚Äù}\\n\\n\\non_prompt_end\\n[template_name]\\n\\n{‚Äúquestion‚Äù: ‚Äúhello‚Äù}\\nChatPromptValue(messages: [SystemMessage, ‚Ä¶])\\n\\n\\n\\n\\nIn addition to the standard events, users can also dispatch custom events (see example below).\\nCustom events will be only be surfaced with in the v2 version of the API!\\nA custom event has following format:\\n\\n\\nAttribute\\nType\\nDescription\\n\\n\\n\\nname\\nstr\\nA user defined name for the event.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='Attribute\\nType\\nDescription\\n\\n\\n\\nname\\nstr\\nA user defined name for the event.\\n\\ndata\\nAny\\nThe data associated with the event. This can be anything, though we suggest making it JSON serializable.\\n\\n\\n\\n\\nHere are declarations associated with the standard events shown above:\\nformat_docs:\\ndef format_docs(docs: List[Document]) -> str:\\n    \\'\\'\\'Format the docs.\\'\\'\\'\\n    return \", \".join([doc.page_content for doc in docs])\\n\\nformat_docs = RunnableLambda(format_docs)\\n\\n\\nsome_tool:\\n@tool\\ndef some_tool(x: int, y: str) -> dict:\\n    \\'\\'\\'Some_tool.\\'\\'\\'\\n    return {\"x\": x, \"y\": y}\\n\\n\\nprompt:\\ntemplate = ChatPromptTemplate.from_messages(\\n    [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\\n).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\\n\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\n\\nasync def reverse(s: str) -> str:\\n    return s[::-1]\\n\\nchain = RunnableLambda(func=reverse)\\n\\nevents = [\\n    event async for event in chain.astream_events(\"hello\", version=\"v2\")\\n]'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='async def reverse(s: str) -> str:\\n    return s[::-1]\\n\\nchain = RunnableLambda(func=reverse)\\n\\nevents = [\\n    event async for event in chain.astream_events(\"hello\", version=\"v2\")\\n]\\n\\n# will produce the following events (run_id, and parent_ids\\n# has been omitted for brevity):\\n[\\n    {\\n        \"data\": {\"input\": \"hello\"},\\n        \"event\": \"on_chain_start\",\\n        \"metadata\": {},\\n        \"name\": \"reverse\",\\n        \"tags\": [],\\n    },\\n    {\\n        \"data\": {\"chunk\": \"olleh\"},\\n        \"event\": \"on_chain_stream\",\\n        \"metadata\": {},\\n        \"name\": \"reverse\",\\n        \"tags\": [],\\n    },\\n    {\\n        \"data\": {\"output\": \"olleh\"},\\n        \"event\": \"on_chain_end\",\\n        \"metadata\": {},\\n        \"name\": \"reverse\",\\n        \"tags\": [],\\n    },\\n]\\n\\n\\nExample: Dispatch Custom Event\\nfrom langchain_core.callbacks.manager import (\\n    adispatch_custom_event,\\n)\\nfrom langchain_core.runnables import RunnableLambda, RunnableConfig\\nimport asyncio'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='Example: Dispatch Custom Event\\nfrom langchain_core.callbacks.manager import (\\n    adispatch_custom_event,\\n)\\nfrom langchain_core.runnables import RunnableLambda, RunnableConfig\\nimport asyncio\\n\\n\\nasync def slow_thing(some_input: str, config: RunnableConfig) -> str:\\n    \"\"\"Do something that takes a long time.\"\"\"\\n    await asyncio.sleep(1) # Placeholder for some slow operation\\n    await adispatch_custom_event(\\n        \"progress_event\",\\n        {\"message\": \"Finished step 1 of 3\"},\\n        config=config # Must be included for python < 3.10\\n    )\\n    await asyncio.sleep(1) # Placeholder for some slow operation\\n    await adispatch_custom_event(\\n        \"progress_event\",\\n        {\"message\": \"Finished step 2 of 3\"},\\n        config=config # Must be included for python < 3.10\\n    )\\n    await asyncio.sleep(1) # Placeholder for some slow operation\\n    return \"Done\"\\n\\nslow_thing = RunnableLambda(slow_thing)\\n\\nasync for event in slow_thing.astream_events(\"some_input\", version=\"v2\"):\\n    print(event)'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='slow_thing = RunnableLambda(slow_thing)\\n\\nasync for event in slow_thing.astream_events(\"some_input\", version=\"v2\"):\\n    print(event)\\n\\n\\n\\nParameters:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content=\"input (Any) ‚Äì The input to the Runnable.\\nconfig (RunnableConfig | None) ‚Äì The config to use for the Runnable.\\nversion (Literal['v1', 'v2']) ‚Äì The version of the schema to use either v2 or v1.\\nUsers should use v2.\\nv1 is for backwards compatibility and will be deprecated\\nin 0.4.0.\\nNo default will be assigned until the API is stabilized.\\ncustom events will only be surfaced in v2.\\ninclude_names (Sequence[str] | None) ‚Äì Only include events from runnables with matching names.\\ninclude_types (Sequence[str] | None) ‚Äì Only include events from runnables with matching types.\\ninclude_tags (Sequence[str] | None) ‚Äì Only include events from runnables with matching tags.\\nexclude_names (Sequence[str] | None) ‚Äì Exclude events from runnables with matching names.\\nexclude_types (Sequence[str] | None) ‚Äì Exclude events from runnables with matching types.\\nexclude_tags (Sequence[str] | None) ‚Äì Exclude events from runnables with matching tags.\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='exclude_types (Sequence[str] | None) ‚Äì Exclude events from runnables with matching types.\\nexclude_tags (Sequence[str] | None) ‚Äì Exclude events from runnables with matching tags.\\nkwargs (Any) ‚Äì Additional keyword arguments to pass to the Runnable.\\nThese will be passed to astream_log as this implementation\\nof astream_events is built on top of astream_log.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='Yields:\\nAn async stream of StreamEvents.\\n\\nRaises:\\nNotImplementedError ‚Äì If the version is not v1 or v2.\\n\\nReturn type:\\nAsyncIterator[StandardStreamEvent | CustomStreamEvent]\\n\\n\\n\\n\\n\\nbatch(inputs: list[Input], config: RunnableConfig | list[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None) ‚Üí list[Output]#\\nDefault implementation runs invoke in parallel using a thread pool executor.\\nThe default implementation of batch works well for IO bound runnables.\\nSubclasses should override this method if they can batch more efficiently;\\ne.g., if the underlying Runnable uses an API which supports a batch mode.\\n\\nParameters:\\n\\ninputs (list[Input])\\nconfig (RunnableConfig | list[RunnableConfig] | None)\\nreturn_exceptions (bool)\\nkwargs (Any | None)\\n\\n\\nReturn type:\\nlist[Output]'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='Parameters:\\n\\ninputs (list[Input])\\nconfig (RunnableConfig | list[RunnableConfig] | None)\\nreturn_exceptions (bool)\\nkwargs (Any | None)\\n\\n\\nReturn type:\\nlist[Output]\\n\\n\\n\\n\\n\\nbatch_as_completed(inputs: Sequence[Input], config: RunnableConfig | Sequence[RunnableConfig] | None = None, *, return_exceptions: bool = False, **kwargs: Any | None) ‚Üí Iterator[tuple[int, Output | Exception]]#\\nRun invoke in parallel on a list of inputs,\\nyielding results as they complete.\\n\\nParameters:\\n\\ninputs (Sequence[Input])\\nconfig (RunnableConfig | Sequence[RunnableConfig] | None)\\nreturn_exceptions (bool)\\nkwargs (Any | None)\\n\\n\\nReturn type:\\nIterator[tuple[int, Output | Exception]]\\n\\n\\n\\n\\n\\nbind(**kwargs: Any) ‚Üí Runnable[Input, Output]#\\nBind arguments to a Runnable, returning a new Runnable.\\nUseful when a Runnable in a chain requires an argument that is not\\nin the output of the previous Runnable or included in the user input.\\n\\nParameters:\\nkwargs (Any) ‚Äì The arguments to bind to the Runnable.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='Parameters:\\nkwargs (Any) ‚Äì The arguments to bind to the Runnable.\\n\\nReturns:\\nA new Runnable with the arguments bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\nExample:\\nfrom langchain_community.chat_models import ChatOllama\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nllm = ChatOllama(model=\\'llama2\\')\\n\\n# Without bind.\\nchain = (\\n    llm\\n    | StrOutputParser()\\n)\\n\\nchain.invoke(\"Repeat quoted words exactly: \\'One two three four five.\\'\")\\n# Output is \\'One two three four five.\\'\\n\\n# With bind.\\nchain = (\\n    llm.bind(stop=[\"three\"])\\n    | StrOutputParser()\\n)\\n\\nchain.invoke(\"Repeat quoted words exactly: \\'One two three four five.\\'\")\\n# Output is \\'One two\\'\\n\\n\\n\\n\\n\\nconfigurable_alternatives(which: ConfigurableField, *, default_key: str = \\'default\\', prefix_keys: bool = False, **kwargs: Runnable[Input, Output] | Callable[[], Runnable[Input, Output]]) ‚Üí RunnableSerializable#\\nConfigure alternatives for Runnables that can be set at runtime.\\n\\nParameters:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='Parameters:\\n\\nwhich (ConfigurableField) ‚Äì The ConfigurableField instance that will be used to select the\\nalternative.\\ndefault_key (str) ‚Äì The default key to use if no alternative is selected.\\nDefaults to ‚Äúdefault‚Äù.\\nprefix_keys (bool) ‚Äì Whether to prefix the keys with the ConfigurableField id.\\nDefaults to False.\\n**kwargs (Runnable[Input, Output] | Callable[[], Runnable[Input, Output]]) ‚Äì A dictionary of keys to Runnable instances or callables that\\nreturn Runnable instances.\\n\\n\\nReturns:\\nA new Runnable with the alternatives configured.\\n\\nReturn type:\\nRunnableSerializable\\n\\n\\nfrom langchain_anthropic import ChatAnthropic\\nfrom langchain_core.runnables.utils import ConfigurableField\\nfrom langchain_openai import ChatOpenAI\\n\\nmodel = ChatAnthropic(\\n    model_name=\"claude-3-sonnet-20240229\"\\n).configurable_alternatives(\\n    ConfigurableField(id=\"llm\"),\\n    default_key=\"anthropic\",\\n    openai=ChatOpenAI()\\n)'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='model = ChatAnthropic(\\n    model_name=\"claude-3-sonnet-20240229\"\\n).configurable_alternatives(\\n    ConfigurableField(id=\"llm\"),\\n    default_key=\"anthropic\",\\n    openai=ChatOpenAI()\\n)\\n\\n# uses the default model ChatAnthropic\\nprint(model.invoke(\"which organization created you?\").content)\\n\\n# uses ChatOpenAI\\nprint(\\n    model.with_config(\\n        configurable={\"llm\": \"openai\"}\\n    ).invoke(\"which organization created you?\").content\\n)\\n\\n\\n\\n\\n\\nconfigurable_fields(**kwargs: ConfigurableField | ConfigurableFieldSingleOption | ConfigurableFieldMultiOption) ‚Üí RunnableSerializable#\\nConfigure particular Runnable fields at runtime.\\n\\nParameters:\\n**kwargs (ConfigurableField | ConfigurableFieldSingleOption | ConfigurableFieldMultiOption) ‚Äì A dictionary of ConfigurableField instances to configure.\\n\\nReturns:\\nA new Runnable with the fields configured.\\n\\nReturn type:\\nRunnableSerializable\\n\\n\\nfrom langchain_core.runnables import ConfigurableField\\nfrom langchain_openai import ChatOpenAI'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='Returns:\\nA new Runnable with the fields configured.\\n\\nReturn type:\\nRunnableSerializable\\n\\n\\nfrom langchain_core.runnables import ConfigurableField\\nfrom langchain_openai import ChatOpenAI\\n\\nmodel = ChatOpenAI(max_tokens=20).configurable_fields(\\n    max_tokens=ConfigurableField(\\n        id=\"output_token_number\",\\n        name=\"Max tokens in the output\",\\n        description=\"The maximum number of tokens in the output\",\\n    )\\n)\\n\\n# max_tokens = 20\\nprint(\\n    \"max_tokens_20: \",\\n    model.invoke(\"tell me something about chess\").content\\n)\\n\\n# max_tokens = 200\\nprint(\"max_tokens_200: \", model.with_config(\\n    configurable={\"output_token_number\": 200}\\n    ).invoke(\"tell me something about chess\").content\\n)\\n\\n\\n\\n\\n\\nextend(messages: Sequence[BaseMessagePromptTemplate | BaseMessage | BaseChatPromptTemplate | tuple[str | type, str | list[dict] | list[object]] | str]) ‚Üí None[source]#\\nExtend the chat template with a sequence of messages.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='Parameters:\\nmessages (Sequence[BaseMessagePromptTemplate | BaseMessage | BaseChatPromptTemplate | tuple[str | type, str | list[dict] | list[object]] | str]) ‚Äì sequence of message representations to append.\\n\\nReturn type:\\nNone\\n\\n\\n\\n\\n\\nformat(**kwargs: Any) ‚Üí str#\\nFormat the chat template into a string.\\n\\nParameters:\\n**kwargs (Any) ‚Äì keyword arguments to use for filling in template variables\\nin all the template messages in this chat template.\\n\\nReturns:\\nformatted string.\\n\\nReturn type:\\nstr\\n\\n\\n\\n\\n\\nformat_messages(**kwargs: Any) ‚Üí list[BaseMessage][source]#\\nFormat the chat template into a list of finalized messages.\\n\\nParameters:\\n**kwargs (Any) ‚Äì keyword arguments to use for filling in template variables\\nin all the template messages in this chat template.\\n\\nReturns:\\nlist of formatted messages.\\n\\nReturn type:\\nlist[BaseMessage]\\n\\n\\n\\n\\n\\nformat_prompt(**kwargs: Any) ‚Üí PromptValue#\\nFormat prompt. Should return a PromptValue.\\n\\nParameters:\\n**kwargs (Any) ‚Äì Keyword arguments to use for formatting.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='Return type:\\nlist[BaseMessage]\\n\\n\\n\\n\\n\\nformat_prompt(**kwargs: Any) ‚Üí PromptValue#\\nFormat prompt. Should return a PromptValue.\\n\\nParameters:\\n**kwargs (Any) ‚Äì Keyword arguments to use for formatting.\\n\\nReturns:\\nPromptValue.\\n\\nReturn type:\\nPromptValue\\n\\n\\n\\n\\n\\nclassmethod from_messages(messages: Sequence[BaseMessagePromptTemplate | BaseMessage | BaseChatPromptTemplate | tuple[str | type, str | list[dict] | list[object]] | str], template_format: Literal[\\'f-string\\', \\'mustache\\', \\'jinja2\\'] = \\'f-string\\') ‚Üí ChatPromptTemplate[source]#\\nCreate a chat prompt template from a variety of message formats.\\nExamples\\nInstantiation from a list of message templates:\\ntemplate = ChatPromptTemplate.from_messages([\\n    (\"human\", \"Hello, how are you?\"),\\n    (\"ai\", \"I\\'m doing well, thanks!\"),\\n    (\"human\", \"That\\'s good to hear.\"),\\n])\\n\\n\\nInstantiation from mixed message formats:\\ntemplate = ChatPromptTemplate.from_messages([\\n    SystemMessage(content=\"hello\"),\\n    (\"human\", \"Hello, how are you?\"),\\n])\\n\\n\\n\\nParameters:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='Instantiation from mixed message formats:\\ntemplate = ChatPromptTemplate.from_messages([\\n    SystemMessage(content=\"hello\"),\\n    (\"human\", \"Hello, how are you?\"),\\n])\\n\\n\\n\\nParameters:\\n\\nmessages (Sequence[BaseMessagePromptTemplate | BaseMessage | BaseChatPromptTemplate | tuple[str | type, str | list[dict] | list[object]] | str]) ‚Äì sequence of message representations.\\nA message can be represented using the following formats:\\n(1) BaseMessagePromptTemplate, (2) BaseMessage, (3) 2-tuple of\\n(message type, template); e.g., (‚Äúhuman‚Äù, ‚Äú{user_input}‚Äù),\\n(4) 2-tuple of (message class, template), (5) a string which is\\nshorthand for (‚Äúhuman‚Äù, template); e.g., ‚Äú{user_input}‚Äù.\\ntemplate_format (Literal[\\'f-string\\', \\'mustache\\', \\'jinja2\\']) ‚Äì format of the template. Defaults to ‚Äúf-string‚Äù.\\n\\n\\nReturns:\\na chat prompt template.\\n\\nReturn type:\\nChatPromptTemplate\\n\\n\\n\\n\\n\\nclassmethod from_role_strings(string_messages: list[tuple[str, str]]) ‚Üí ChatPromptTemplate[source]#'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='Returns:\\na chat prompt template.\\n\\nReturn type:\\nChatPromptTemplate\\n\\n\\n\\n\\n\\nclassmethod from_role_strings(string_messages: list[tuple[str, str]]) ‚Üí ChatPromptTemplate[source]#\\n\\nDeprecated since version 0.0.1: Use from_messages classmethod() instead.\\n\\nCreate a chat prompt template from a list of (role, template) tuples.\\n\\nParameters:\\nstring_messages (list[tuple[str, str]]) ‚Äì list of (role, template) tuples.\\n\\nReturns:\\na chat prompt template.\\n\\nReturn type:\\nChatPromptTemplate\\n\\n\\n\\n\\n\\nclassmethod from_strings(string_messages: list[tuple[type[BaseMessagePromptTemplate], str]]) ‚Üí ChatPromptTemplate[source]#\\n\\nDeprecated since version 0.0.1: Use from_messages classmethod() instead.\\n\\nCreate a chat prompt template from a list of (role class, template) tuples.\\n\\nParameters:\\nstring_messages (list[tuple[type[BaseMessagePromptTemplate], str]]) ‚Äì list of (role class, template) tuples.\\n\\nReturns:\\na chat prompt template.\\n\\nReturn type:\\nChatPromptTemplate'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='Parameters:\\nstring_messages (list[tuple[type[BaseMessagePromptTemplate], str]]) ‚Äì list of (role class, template) tuples.\\n\\nReturns:\\na chat prompt template.\\n\\nReturn type:\\nChatPromptTemplate\\n\\n\\n\\n\\n\\nclassmethod from_template(template: str, **kwargs: Any) ‚Üí ChatPromptTemplate[source]#\\nCreate a chat prompt template from a template string.\\nCreates a chat template consisting of a single message assumed to be from\\nthe human.\\n\\nParameters:\\n\\ntemplate (str) ‚Äì template string\\n**kwargs (Any) ‚Äì keyword arguments to pass to the constructor.\\n\\n\\nReturns:\\nA new instance of this class.\\n\\nReturn type:\\nChatPromptTemplate\\n\\n\\n\\n\\n\\ninvoke(input: dict, config: RunnableConfig | None = None, **kwargs: Any) ‚Üí PromptValue#\\nInvoke the prompt.\\n\\nParameters:\\n\\ninput (dict) ‚Äì Dict, input to the prompt.\\nconfig (RunnableConfig | None) ‚Äì RunnableConfig, configuration for the prompt.\\nkwargs (Any)\\n\\n\\nReturns:\\nThe output of the prompt.\\n\\nReturn type:\\nPromptValue'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='Returns:\\nThe output of the prompt.\\n\\nReturn type:\\nPromptValue\\n\\n\\n\\n\\n\\npartial(**kwargs: Any) ‚Üí ChatPromptTemplate[source]#\\nGet a new ChatPromptTemplate with some input variables already filled in.\\n\\nParameters:\\n**kwargs (Any) ‚Äì keyword arguments to use for filling in template variables. Ought\\nto be a subset of the input variables.\\n\\nReturns:\\nA new ChatPromptTemplate.\\n\\nReturn type:\\nChatPromptTemplate\\n\\n\\nExample\\nfrom langchain_core.prompts import ChatPromptTemplate\\n\\ntemplate = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", \"You are an AI assistant named {name}.\"),\\n        (\"human\", \"Hi I\\'m {user}\"),\\n        (\"ai\", \"Hi there, {user}, I\\'m {name}.\"),\\n        (\"human\", \"{input}\"),\\n    ]\\n)\\ntemplate2 = template.partial(user=\"Lucy\", name=\"R2D2\")\\n\\ntemplate2.format_messages(input=\"hello\")\\n\\n\\n\\n\\n\\npretty_print() ‚Üí None#\\nPrint a human-readable representation.\\n\\nReturn type:\\nNone\\n\\n\\n\\n\\n\\npretty_repr(html: bool = False) ‚Üí str[source]#\\nHuman-readable representation.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='pretty_print() ‚Üí None#\\nPrint a human-readable representation.\\n\\nReturn type:\\nNone\\n\\n\\n\\n\\n\\npretty_repr(html: bool = False) ‚Üí str[source]#\\nHuman-readable representation.\\n\\nParameters:\\nhtml (bool) ‚Äì Whether to format as HTML. Defaults to False.\\n\\nReturns:\\nHuman-readable representation.\\n\\nReturn type:\\nstr\\n\\n\\n\\n\\n\\nsave(file_path: Path | str) ‚Üí None[source]#\\nSave prompt to file.\\n\\nParameters:\\nfile_path (Path | str) ‚Äì path to file.\\n\\nReturn type:\\nNone\\n\\n\\n\\n\\n\\nstream(input: Input, config: RunnableConfig | None = None, **kwargs: Any | None) ‚Üí Iterator[Output]#\\nDefault implementation of stream, which calls invoke.\\nSubclasses should override this method if they support streaming output.\\n\\nParameters:\\n\\ninput (Input) ‚Äì The input to the Runnable.\\nconfig (RunnableConfig | None) ‚Äì The config to use for the Runnable. Defaults to None.\\nkwargs (Any | None) ‚Äì Additional keyword arguments to pass to the Runnable.\\n\\n\\nYields:\\nThe output of the Runnable.\\n\\nReturn type:\\nIterator[Output]'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='Yields:\\nThe output of the Runnable.\\n\\nReturn type:\\nIterator[Output]\\n\\n\\n\\n\\n\\nwith_alisteners(*, on_start: AsyncListener | None = None, on_end: AsyncListener | None = None, on_error: AsyncListener | None = None) ‚Üí Runnable[Input, Output]#\\nBind async lifecycle listeners to a Runnable, returning a new Runnable.\\non_start: Asynchronously called before the Runnable starts running.\\non_end: Asynchronously called after the Runnable finishes running.\\non_error: Asynchronously called if the Runnable throws an error.\\nThe Run object contains information about the run, including its id,\\ntype, input, output, error, start_time, end_time, and any tags or metadata\\nadded to the run.\\n\\nParameters:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='Parameters:\\n\\non_start (Optional[AsyncListener]) ‚Äì Asynchronously called before the Runnable starts running.\\nDefaults to None.\\non_end (Optional[AsyncListener]) ‚Äì Asynchronously called after the Runnable finishes running.\\nDefaults to None.\\non_error (Optional[AsyncListener]) ‚Äì Asynchronously called if the Runnable throws an error.\\nDefaults to None.\\n\\n\\nReturns:\\nA new Runnable with the listeners bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\nimport time\\n\\nasync def test_runnable(time_to_sleep : int):\\n    print(f\"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}\")\\n    await asyncio.sleep(time_to_sleep)\\n    print(f\"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}\")\\n\\nasync def fn_start(run_obj : Runnable):\\n    print(f\"on start callback starts at {format_t(time.time())}\\n    await asyncio.sleep(3)\\n    print(f\"on start callback ends at {format_t(time.time())}\")'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='async def fn_start(run_obj : Runnable):\\n    print(f\"on start callback starts at {format_t(time.time())}\\n    await asyncio.sleep(3)\\n    print(f\"on start callback ends at {format_t(time.time())}\")\\n\\nasync def fn_end(run_obj : Runnable):\\n    print(f\"on end callback starts at {format_t(time.time())}\\n    await asyncio.sleep(2)\\n    print(f\"on end callback ends at {format_t(time.time())}\")\\n\\nrunnable = RunnableLambda(test_runnable).with_alisteners(\\n    on_start=fn_start,\\n    on_end=fn_end\\n)\\nasync def concurrent_runs():\\n    await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='runnable = RunnableLambda(test_runnable).with_alisteners(\\n    on_start=fn_start,\\n    on_end=fn_end\\n)\\nasync def concurrent_runs():\\n    await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))\\n\\nasyncio.run(concurrent_runs())\\nResult:\\non start callback starts at 2024-05-16T14:20:29.637053+00:00\\non start callback starts at 2024-05-16T14:20:29.637150+00:00\\non start callback ends at 2024-05-16T14:20:32.638305+00:00\\non start callback ends at 2024-05-16T14:20:32.638383+00:00\\nRunnable[3s]: starts at 2024-05-16T14:20:32.638849+00:00\\nRunnable[5s]: starts at 2024-05-16T14:20:32.638999+00:00\\nRunnable[3s]: ends at 2024-05-16T14:20:35.640016+00:00\\non end callback starts at 2024-05-16T14:20:35.640534+00:00\\nRunnable[5s]: ends at 2024-05-16T14:20:37.640169+00:00\\non end callback starts at 2024-05-16T14:20:37.640574+00:00\\non end callback ends at 2024-05-16T14:20:37.640654+00:00\\non end callback ends at 2024-05-16T14:20:39.641751+00:00'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content=\"with_config(config: RunnableConfig | None = None, **kwargs: Any) ‚Üí Runnable[Input, Output]#\\nBind config to a Runnable, returning a new Runnable.\\n\\nParameters:\\n\\nconfig (RunnableConfig | None) ‚Äì The config to bind to the Runnable.\\nkwargs (Any) ‚Äì Additional keyword arguments to pass to the Runnable.\\n\\n\\nReturns:\\nA new Runnable with the config bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\n\\n\\n\\nwith_fallbacks(fallbacks: Sequence[Runnable[Input, Output]], *, exceptions_to_handle: tuple[type[BaseException], ...] = (<class 'Exception'>,), exception_key: Optional[str] = None) ‚Üí RunnableWithFallbacksT[Input, Output]#\\nAdd fallbacks to a Runnable, returning a new Runnable.\\nThe new Runnable will try the original Runnable, and then each fallback\\nin order, upon failures.\\n\\nParameters:\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='Parameters:\\n\\nfallbacks (Sequence[Runnable[Input, Output]]) ‚Äì A sequence of runnables to try if the original Runnable fails.\\nexceptions_to_handle (tuple[type[BaseException], ...]) ‚Äì A tuple of exception types to handle.\\nDefaults to (Exception,).\\nexception_key (Optional[str]) ‚Äì If string is specified then handled exceptions will be passed\\nto fallbacks as part of the input under the specified key. If None,\\nexceptions will not be passed to fallbacks. If used, the base Runnable\\nand its fallbacks must accept a dictionary as input. Defaults to None.\\n\\n\\nReturns:\\nA new Runnable that will try the original Runnable, and then each\\nfallback in order, upon failures.\\n\\nReturn type:\\nRunnableWithFallbacksT[Input, Output]\\n\\n\\nExample\\nfrom typing import Iterator\\n\\nfrom langchain_core.runnables import RunnableGenerator\\n\\n\\ndef _generate_immediate_error(input: Iterator) -> Iterator[str]:\\n    raise ValueError()\\n    yield \"\"\\n\\n\\ndef _generate(input: Iterator) -> Iterator[str]:\\n    yield from \"foo bar\"'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='def _generate_immediate_error(input: Iterator) -> Iterator[str]:\\n    raise ValueError()\\n    yield \"\"\\n\\n\\ndef _generate(input: Iterator) -> Iterator[str]:\\n    yield from \"foo bar\"\\n\\n\\nrunnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(\\n    [RunnableGenerator(_generate)]\\n    )\\nprint(\\'\\'.join(runnable.stream({}))) #foo bar\\n\\n\\n\\nParameters:\\n\\nfallbacks (Sequence[Runnable[Input, Output]]) ‚Äì A sequence of runnables to try if the original Runnable fails.\\nexceptions_to_handle (tuple[type[BaseException], ...]) ‚Äì A tuple of exception types to handle.\\nexception_key (Optional[str]) ‚Äì If string is specified then handled exceptions will be passed\\nto fallbacks as part of the input under the specified key. If None,\\nexceptions will not be passed to fallbacks. If used, the base Runnable\\nand its fallbacks must accept a dictionary as input.\\n\\n\\nReturns:\\nA new Runnable that will try the original Runnable, and then each\\nfallback in order, upon failures.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='Returns:\\nA new Runnable that will try the original Runnable, and then each\\nfallback in order, upon failures.\\n\\nReturn type:\\nRunnableWithFallbacksT[Input, Output]\\n\\n\\n\\n\\n\\nwith_listeners(*, on_start: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None, on_end: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None, on_error: Callable[[Run], None] | Callable[[Run, RunnableConfig], None] | None = None) ‚Üí Runnable[Input, Output]#\\nBind lifecycle listeners to a Runnable, returning a new Runnable.\\non_start: Called before the Runnable starts running, with the Run object.\\non_end: Called after the Runnable finishes running, with the Run object.\\non_error: Called if the Runnable throws an error, with the Run object.\\nThe Run object contains information about the run, including its id,\\ntype, input, output, error, start_time, end_time, and any tags or metadata\\nadded to the run.\\n\\nParameters:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='Parameters:\\n\\non_start (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) ‚Äì Called before the Runnable starts running. Defaults to None.\\non_end (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) ‚Äì Called after the Runnable finishes running. Defaults to None.\\non_error (Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]) ‚Äì Called if the Runnable throws an error. Defaults to None.\\n\\n\\nReturns:\\nA new Runnable with the listeners bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\nfrom langchain_core.tracers.schemas import Run\\n\\nimport time\\n\\ndef test_runnable(time_to_sleep : int):\\n    time.sleep(time_to_sleep)\\n\\ndef fn_start(run_obj: Run):\\n    print(\"start_time:\", run_obj.start_time)\\n\\ndef fn_end(run_obj: Run):\\n    print(\"end_time:\", run_obj.end_time)'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='def fn_start(run_obj: Run):\\n    print(\"start_time:\", run_obj.start_time)\\n\\ndef fn_end(run_obj: Run):\\n    print(\"end_time:\", run_obj.end_time)\\n\\nchain = RunnableLambda(test_runnable).with_listeners(\\n    on_start=fn_start,\\n    on_end=fn_end\\n)\\nchain.invoke(2)\\n\\n\\n\\n\\n\\nwith_retry(*, retry_if_exception_type: tuple[type[BaseException], ...] = (<class \\'Exception\\'>,), wait_exponential_jitter: bool = True, stop_after_attempt: int = 3) ‚Üí Runnable[Input, Output]#\\nCreate a new Runnable that retries the original Runnable on exceptions.\\n\\nParameters:\\n\\nretry_if_exception_type (tuple[type[BaseException], ...]) ‚Äì A tuple of exception types to retry on.\\nDefaults to (Exception,).\\nwait_exponential_jitter (bool) ‚Äì Whether to add jitter to the wait\\ntime between retries. Defaults to True.\\nstop_after_attempt (int) ‚Äì The maximum number of attempts to make before\\ngiving up. Defaults to 3.\\n\\n\\nReturns:\\nA new Runnable that retries the original Runnable on exceptions.\\n\\nReturn type:\\nRunnable[Input, Output]'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='Returns:\\nA new Runnable that retries the original Runnable on exceptions.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\nExample:\\nfrom langchain_core.runnables import RunnableLambda\\n\\ncount = 0\\n\\n\\ndef _lambda(x: int) -> None:\\n    global count\\n    count = count + 1\\n    if x == 1:\\n        raise ValueError(\"x is 1\")\\n    else:\\n         pass\\n\\n\\nrunnable = RunnableLambda(_lambda)\\ntry:\\n    runnable.with_retry(\\n        stop_after_attempt=2,\\n        retry_if_exception_type=(ValueError,),\\n    ).invoke(1)\\nexcept ValueError:\\n    pass\\n\\nassert (count == 2)\\n\\n\\n\\nParameters:\\n\\nretry_if_exception_type (tuple[type[BaseException], ...]) ‚Äì A tuple of exception types to retry on\\nwait_exponential_jitter (bool) ‚Äì Whether to add jitter to the wait time\\nbetween retries\\nstop_after_attempt (int) ‚Äì The maximum number of attempts to make before giving up\\n\\n\\nReturns:\\nA new Runnable that retries the original Runnable on exceptions.\\n\\nReturn type:\\nRunnable[Input, Output]'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='Returns:\\nA new Runnable that retries the original Runnable on exceptions.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\n\\n\\n\\nwith_types(*, input_type: type[Input] | None = None, output_type: type[Output] | None = None) ‚Üí Runnable[Input, Output]#\\nBind input and output types to a Runnable, returning a new Runnable.\\n\\nParameters:\\n\\ninput_type (type[Input] | None) ‚Äì The input type to bind to the Runnable. Defaults to None.\\noutput_type (type[Output] | None) ‚Äì The output type to bind to the Runnable. Defaults to None.\\n\\n\\nReturns:\\nA new Runnable with the types bound.\\n\\nReturn type:\\nRunnable[Input, Output]\\n\\n\\n\\n\\nExamples using ChatPromptTemplate'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='# Basic example (short documents)\\n# Example\\n# Legacy\\nAWS DynamoDB\\nActiveloop Deep Memory\\nApache Cassandra\\nApertureDB\\nArxivRetriever\\nAskNews\\nAzureAISearchRetriever\\nAzureChatOpenAI\\nBuild a Chatbot\\nBuild a Local RAG Application\\nBuild a PDF ingestion and Question/Answering system\\nBuild a Query Analysis System\\nBuild a Retrieval Augmented Generation (RAG) App\\nBuild a Simple LLM Application with LCEL\\nBuild an Extraction Chain\\nChatAI21\\nChatAnthropic\\nChatBedrock\\nChatDatabricks\\nChatFireworks\\nChatGoogleGenerativeAI\\nChatGroq\\nChatMistralAI\\nChatNVIDIA\\nChatOCIGenAI\\nChatOllama\\nChatOpenAI\\nChatPerplexity\\nChatTogether\\nChatUpstage\\nChatVertexAI\\nChatWatsonx\\nChatYI\\nClassify Text into Labels\\nCohere\\nConceptual guide\\nContext\\nConversational RAG\\nCouchbase\\nDatabricks Unity Catalog (UC)\\nEden AI\\nElasticsearchRetriever\\nFacebook Messenger\\nFiddler\\nFigma\\nFinancialDatasets Toolkit\\nFleet AI Context\\nGoogle AlloyDB for PostgreSQL\\nGoogle El Carro Oracle\\nGoogle SQL for MySQL\\nGoogle SQL for PostgreSQL'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='ElasticsearchRetriever\\nFacebook Messenger\\nFiddler\\nFigma\\nFinancialDatasets Toolkit\\nFleet AI Context\\nGoogle AlloyDB for PostgreSQL\\nGoogle El Carro Oracle\\nGoogle SQL for MySQL\\nGoogle SQL for PostgreSQL\\nGoogle SQL for SQL Server\\nGoogle Vertex AI Search\\nHow deal with high cardinality categoricals when doing query analysis\\nHow to add a semantic layer over graph database\\nHow to add ad-hoc tool calling capability to LLMs and Chat Models\\nHow to add chat history\\nHow to add default invocation args to a Runnable\\nHow to add examples to the prompt for query analysis\\nHow to add fallbacks to a runnable\\nHow to add memory to chatbots\\nHow to add message history\\nHow to add retrieval to chatbots\\nHow to add tools to chatbots\\nHow to add values to a chain‚Äôs state\\nHow to attach callbacks to a runnable\\nHow to chain runnables\\nHow to convert Runnables as Tools\\nHow to create a custom LLM class\\nHow to create a dynamic (self-constructing) chain\\nHow to create custom callback handlers\\nHow to create tools'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='How to chain runnables\\nHow to convert Runnables as Tools\\nHow to create a custom LLM class\\nHow to create a dynamic (self-constructing) chain\\nHow to create custom callback handlers\\nHow to create tools\\nHow to deal with large databases when doing SQL question-answering\\nHow to debug your LLM apps\\nHow to do per-user retrieval\\nHow to do query validation as part of SQL question-answering\\nHow to do question answering over CSVs\\nHow to do tool/function calling\\nHow to get a RAG application to add citations\\nHow to get your RAG application to return sources\\nHow to handle cases where no queries are generated\\nHow to handle long text when doing extraction\\nHow to handle multiple queries when doing query analysis\\nHow to handle multiple retrievers when doing query analysis\\nHow to handle tool errors\\nHow to inspect runnables\\nHow to invoke runnables in parallel\\nHow to map values to a graph database\\nHow to migrate from legacy LangChain agents to LangGraph\\nHow to pass callbacks in at runtime'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='How to inspect runnables\\nHow to invoke runnables in parallel\\nHow to map values to a graph database\\nHow to migrate from legacy LangChain agents to LangGraph\\nHow to pass callbacks in at runtime\\nHow to pass through arguments from one step to the next\\nHow to propagate callbacks  constructor\\nHow to retrieve using multiple vectors per document\\nHow to return structured data from a model\\nHow to run custom functions\\nHow to save and load LangChain objects\\nHow to stream events from a tool\\nHow to stream results from your RAG application\\nHow to stream runnables\\nHow to summarize text in a single LLM call\\nHow to summarize text through iterative refinement\\nHow to summarize text through parallelization\\nHow to track token usage in ChatModels\\nHow to use few shot examples in chat models\\nHow to use few-shot prompting with tool calling\\nHow to use multimodal prompts\\nHow to use prompting alone (no tool calling) to do extraction\\nHow to use reference examples when doing extraction\\nHybrid Search\\nImage captions'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='How to use multimodal prompts\\nHow to use prompting alone (no tool calling) to do extraction\\nHow to use reference examples when doing extraction\\nHybrid Search\\nImage captions\\nJaguar Vector Database\\nJinaChat\\nKinetica Language To SQL Chat Model\\nLangChain Expression Language Cheatsheet\\nLangSmith LLM Runs\\nLlama.cpp\\nLlama2Chat\\nLoad docs\\nMLflow\\nMaritalk\\nMongoDB\\nNVIDIA NIMs \\nOllamaFunctions\\nOllamaLLM\\nOpenAI metadata tagger\\nRAGatouille\\nRedis\\nRiza Code Interpreter\\nSQL (SQLAlchemy)\\nSQLite\\nStreamlit\\nSummarize Text\\nTavily Search\\nTavilySearchAPIRetriever\\nTiDB\\nUpTrain\\nVector stores and retrievers\\nWeaviate\\nWikipediaRetriever\\nYellowbrick\\nYou.com\\nYuan2.0\\nZepCloudChatMessageHistory\\niMessage\\nvLLM Chat\\nü¶úÔ∏èüèì LangServe'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html', 'title': 'ChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation', 'language': 'en'}, page_content='On this page\\n  \\n\\n\\nChatPromptTemplate\\ninput_types\\ninput_variables\\nmessages\\nmetadata\\noptional_variables\\noutput_parser\\npartial_variables\\ntags\\nvalidate_template\\nabatch()\\nabatch_as_completed()\\naformat()\\naformat_messages()\\naformat_prompt()\\nainvoke()\\nappend()\\nastream()\\nastream_events()\\nbatch()\\nbatch_as_completed()\\nbind()\\nconfigurable_alternatives()\\nconfigurable_fields()\\nextend()\\nformat()\\nformat_messages()\\nformat_prompt()\\nfrom_messages()\\nfrom_role_strings()\\nfrom_strings()\\nfrom_template()\\ninvoke()\\npartial()\\npretty_print()\\npretty_repr()\\nsave()\\nstream()\\nwith_alisteners()\\nwith_config()\\nwith_fallbacks()\\nwith_listeners()\\nwith_retry()\\nwith_types()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n      ¬© Copyright 2023, LangChain Inc.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(api_key=api_key2,model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstoredb=FAISS.from_documents(documents,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x10e007f40>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##direct query to vector db \n",
    "query = \"Below is a table that illustrates some events that might be emitted by various chains. Metadata fields have been omitted from the table for brevity. Chain definitions have been included after the table.\"\n",
    "result = vectorstoredb.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags: Optional[List[str]] - The tags of the Runnable that generatedthe event.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "metadata: Optional[Dict[str, Any]] - The metadata of the Runnablethat generated the event.\n",
      "\n",
      "\n",
      "\n",
      "data: Dict[str, Any]\n",
      "\n",
      "Below is a table that illustrates some events that might be emitted by various\n",
      "chains. Metadata fields have been omitted from the table for brevity.\n",
      "Chain definitions have been included after the table.\n",
      "ATTENTION This reference table is for the V2 version of the schema.\n",
      "\n",
      "\n",
      "event\n",
      "name\n",
      "chunk\n",
      "input\n",
      "output\n",
      "\n",
      "\n",
      "\n",
      "on_chat_model_start\n",
      "[model name]\n",
      "\n",
      "{‚Äúmessages‚Äù: [[SystemMessage, HumanMessage]]}\n",
      "\n",
      "\n",
      "on_chat_model_stream\n",
      "[model name]\n",
      "AIMessageChunk(content=‚Äùhello‚Äù)\n",
      "\n",
      "\n",
      "\n",
      "on_chat_model_end\n",
      "[model name]\n",
      "\n",
      "{‚Äúmessages‚Äù: [[SystemMessage, HumanMessage]]}\n",
      "AIMessageChunk(content=‚Äùhello world‚Äù)\n",
      "\n",
      "on_llm_start\n",
      "[model name]\n",
      "\n",
      "{‚Äòinput‚Äô: ‚Äòhello‚Äô}\n",
      "\n",
      "\n",
      "on_llm_stream\n",
      "[model name]\n",
      "‚ÄòHello‚Äô\n",
      "\n",
      "\n",
      "\n",
      "on_llm_end\n",
      "[model name]\n",
      "\n",
      "‚ÄòHello human!‚Äô\n",
      "\n",
      "\n",
      "on_chain_start\n",
      "format_docs\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "on_chain_stream\n",
      "format_docs\n",
      "‚Äúhello world!, goodbye world!‚Äù\n"
     ]
    }
   ],
   "source": [
    "print(result[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.Completions object at 0x12671b6a0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x126761000> root_client=<openai.OpenAI object at 0x1266da2c0> root_async_client=<openai.AsyncOpenAI object at 0x12671b940> model_name='gpt-4-turbo-preview' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm  = ChatOpenAI(api_key=api_key2,model=\"gpt-4-turbo-preview\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={'context': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x10843aa70>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an AI assistant that answers questions based on the provided context.'), additional_kwargs={}), MessagesPlaceholder(variable_name='context'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Answer the question based only on the provided context.'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x12671b6a0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x126761000>, root_client=<openai.OpenAI object at 0x1266da2c0>, root_async_client=<openai.AsyncOpenAI object at 0x12671b940>, model_name='gpt-4-turbo-preview', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Retrievel Chain , document chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain #create a chain to pass a list of document to a model\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that answers questions based on the provided context.\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "    (\"assistant\", \"I'll help answer your question based on the context provided.\"),\n",
    "    MessagesPlaceholder(variable_name=\"context\"),\n",
    "])\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm,prompt)\n",
    "\n",
    "document_chain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can use retriever to dynamically select the most relevent documents and pass those in for a given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "###input ---> Retriver ---->vectordb\n",
    "retriever = vectorstoredb.as_retriever()\n",
    "retrieval_chain = (\n",
    "    retriever\n",
    "    | document_chain\n",
    ")\n",
    "from langchain.chains import create_retrieval_chain\n",
    " ##combine retiver and document_chain , retirver for to get relevent data from vectordbs and docs_chain for context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()} \n",
    "    | document_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "variable context should be a list of base messages, got How to chain runnables\nHow to convert Runnables as Tools\nHow to create a custom LLM class\nHow to create a dynamic (self-constructing) chain\nHow to create custom callback handlers\nHow to create tools\nHow to deal with large databases when doing SQL question-answering\nHow to debug your LLM apps\nHow to do per-user retrieval\nHow to do query validation as part of SQL question-answering\nHow to do question answering over CSVs\nHow to do tool/function calling\nHow to get a RAG application to add citations\nHow to get your RAG application to return sources\nHow to handle cases where no queries are generated\nHow to handle long text when doing extraction\nHow to handle multiple queries when doing query analysis\nHow to handle multiple retrievers when doing query analysis\nHow to handle tool errors\nHow to inspect runnables\nHow to invoke runnables in parallel\nHow to map values to a graph database\nHow to migrate from legacy LangChain agents to LangGraph\nHow to pass callbacks in at runtime\n\nHow to inspect runnables\nHow to invoke runnables in parallel\nHow to map values to a graph database\nHow to migrate from legacy LangChain agents to LangGraph\nHow to pass callbacks in at runtime\nHow to pass through arguments from one step to the next\nHow to propagate callbacks  constructor\nHow to retrieve using multiple vectors per document\nHow to return structured data from a model\nHow to run custom functions\nHow to save and load LangChain objects\nHow to stream events from a tool\nHow to stream results from your RAG application\nHow to stream runnables\nHow to summarize text in a single LLM call\nHow to summarize text through iterative refinement\nHow to summarize text through parallelization\nHow to track token usage in ChatModels\nHow to use few shot examples in chat models\nHow to use few-shot prompting with tool calling\nHow to use multimodal prompts\nHow to use prompting alone (no tool calling) to do extraction\nHow to use reference examples when doing extraction\nHybrid Search\nImage captions\n\nChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to main content\n\n\nBack to top\n\n\n\n\nCtrl+K\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Reference\n  \n\n\n\n\n\n\n\n\n\nCtrl+K\n\n\n\n\n\n\n\nDocs\n\n\n\n\n\n\n\n\n\n\nGitHub\n\n\n\nX / Twitter\n\n\n\n\n\n\n\n\nCtrl+K\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Reference\n  \n\n\n\n\n\n\n\n\n\n\nDocs\n\n\n\n\n\n\n\n\n\n\nGitHub\n\n\n\nX / Twitter\n\n\n\n\n\n\n\nSection Navigation\nBase packages\n\nOn this page\n  \n\n\nChatPromptTemplate\ninput_types\ninput_variables\nmessages\nmetadata\noptional_variables\noutput_parser\npartial_variables\ntags\nvalidate_template\nabatch()\nabatch_as_completed()\naformat()\naformat_messages()\naformat_prompt()\nainvoke()\nappend()\nastream()\nastream_events()\nbatch()\nbatch_as_completed()\nbind()\nconfigurable_alternatives()\nconfigurable_fields()\nextend()\nformat()\nformat_messages()\nformat_prompt()\nfrom_messages()\nfrom_role_strings()\nfrom_strings()\nfrom_template()\ninvoke()\npartial()\npretty_print()\npretty_repr()\nsave()\nstream()\nwith_alisteners()\nwith_config()\nwith_fallbacks()\nwith_listeners()\nwith_retry()\nwith_types()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n      ¬© Copyright 2023, LangChain Inc. of type <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m      2\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is LangChain?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrag_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[0;32m~/Developer/Langchain/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:3016\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3014\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3015\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3016\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3017\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3018\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Developer/Langchain/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:5352\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5347\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5348\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   5349\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5350\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5351\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5353\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5354\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5355\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5356\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/Langchain/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:3016\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3014\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3015\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3016\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3017\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3018\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Developer/Langchain/venv/lib/python3.10/site-packages/langchain_core/prompts/base.py:210\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags:\n\u001b[1;32m    209\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_prompt_with_error_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserialized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serialized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/Langchain/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:1914\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[1;32m   1910\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1911\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m   1912\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1913\u001b[0m         Output,\n\u001b[0;32m-> 1914\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1915\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1916\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1917\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1918\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1921\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1922\u001b[0m     )\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1924\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/Developer/Langchain/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    395\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/Langchain/venv/lib/python3.10/site-packages/langchain_core/prompts/base.py:185\u001b[0m, in \u001b[0;36mBasePromptTemplate._format_prompt_with_error_handling\u001b[0;34m(self, inner_input)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[1;32m    184\u001b[0m     _inner_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_input(inner_input)\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_inner_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/Langchain/venv/lib/python3.10/site-packages/langchain_core/prompts/chat.py:783\u001b[0m, in \u001b[0;36mBaseChatPromptTemplate.format_prompt\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mformat_prompt\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[1;32m    775\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt. Should return a PromptValue.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m \n\u001b[1;32m    777\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;124;03m        PromptValue.\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 783\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptValue(messages\u001b[38;5;241m=\u001b[39mmessages)\n",
      "File \u001b[0;32m~/Developer/Langchain/venv/lib/python3.10/site-packages/langchain_core/prompts/chat.py:1224\u001b[0m, in \u001b[0;36mChatPromptTemplate.format_messages\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1220\u001b[0m     result\u001b[38;5;241m.\u001b[39mextend([message_template])\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1222\u001b[0m     message_template, (BaseMessagePromptTemplate, BaseChatPromptTemplate)\n\u001b[1;32m   1223\u001b[0m ):\n\u001b[0;32m-> 1224\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[43mmessage_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1225\u001b[0m     result\u001b[38;5;241m.\u001b[39mextend(message)\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Developer/Langchain/venv/lib/python3.10/site-packages/langchain_core/prompts/chat.py:247\u001b[0m, in \u001b[0;36mMessagesPlaceholder.format_messages\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    243\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should be a list of base messages, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m     )\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)  \u001b[38;5;66;03m# noqa: TRY004\u001b[39;00m\n\u001b[1;32m    248\u001b[0m value \u001b[38;5;241m=\u001b[39m convert_to_messages(value)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_messages:\n",
      "\u001b[0;31mValueError\u001b[0m: variable context should be a list of base messages, got How to chain runnables\nHow to convert Runnables as Tools\nHow to create a custom LLM class\nHow to create a dynamic (self-constructing) chain\nHow to create custom callback handlers\nHow to create tools\nHow to deal with large databases when doing SQL question-answering\nHow to debug your LLM apps\nHow to do per-user retrieval\nHow to do query validation as part of SQL question-answering\nHow to do question answering over CSVs\nHow to do tool/function calling\nHow to get a RAG application to add citations\nHow to get your RAG application to return sources\nHow to handle cases where no queries are generated\nHow to handle long text when doing extraction\nHow to handle multiple queries when doing query analysis\nHow to handle multiple retrievers when doing query analysis\nHow to handle tool errors\nHow to inspect runnables\nHow to invoke runnables in parallel\nHow to map values to a graph database\nHow to migrate from legacy LangChain agents to LangGraph\nHow to pass callbacks in at runtime\n\nHow to inspect runnables\nHow to invoke runnables in parallel\nHow to map values to a graph database\nHow to migrate from legacy LangChain agents to LangGraph\nHow to pass callbacks in at runtime\nHow to pass through arguments from one step to the next\nHow to propagate callbacks  constructor\nHow to retrieve using multiple vectors per document\nHow to return structured data from a model\nHow to run custom functions\nHow to save and load LangChain objects\nHow to stream events from a tool\nHow to stream results from your RAG application\nHow to stream runnables\nHow to summarize text in a single LLM call\nHow to summarize text through iterative refinement\nHow to summarize text through parallelization\nHow to track token usage in ChatModels\nHow to use few shot examples in chat models\nHow to use few-shot prompting with tool calling\nHow to use multimodal prompts\nHow to use prompting alone (no tool calling) to do extraction\nHow to use reference examples when doing extraction\nHybrid Search\nImage captions\n\nChatPromptTemplate ‚Äî ü¶úüîó LangChain  documentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to main content\n\n\nBack to top\n\n\n\n\nCtrl+K\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Reference\n  \n\n\n\n\n\n\n\n\n\nCtrl+K\n\n\n\n\n\n\n\nDocs\n\n\n\n\n\n\n\n\n\n\nGitHub\n\n\n\nX / Twitter\n\n\n\n\n\n\n\n\nCtrl+K\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Reference\n  \n\n\n\n\n\n\n\n\n\n\nDocs\n\n\n\n\n\n\n\n\n\n\nGitHub\n\n\n\nX / Twitter\n\n\n\n\n\n\n\nSection Navigation\nBase packages\n\nOn this page\n  \n\n\nChatPromptTemplate\ninput_types\ninput_variables\nmessages\nmetadata\noptional_variables\noutput_parser\npartial_variables\ntags\nvalidate_template\nabatch()\nabatch_as_completed()\naformat()\naformat_messages()\naformat_prompt()\nainvoke()\nappend()\nastream()\nastream_events()\nbatch()\nbatch_as_completed()\nbind()\nconfigurable_alternatives()\nconfigurable_fields()\nextend()\nformat()\nformat_messages()\nformat_prompt()\nfrom_messages()\nfrom_role_strings()\nfrom_strings()\nfrom_template()\ninvoke()\npartial()\npretty_print()\npretty_repr()\nsave()\nstream()\nwith_alisteners()\nwith_config()\nwith_fallbacks()\nwith_listeners()\nwith_retry()\nwith_types()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n      ¬© Copyright 2023, LangChain Inc. of type <class 'str'>"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"What is LangChain?\"\n",
    "response = rag_chain.invoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
